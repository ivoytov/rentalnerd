{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "# %sql mysql://root@localhost/rental_nerd\n",
    "# %sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "    \n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = dt.date.today()\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "model_path = '/home/ilya/rentalnerd-models/'\n",
    "csv_path = '/home/ilya/Code/rentalnerd/scraper/'\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'reg:linear',\n",
    "         'booster': 'gbtree',\n",
    "         'eval_metric':'mae', \n",
    "         'updater': 'grow_gpu_hist',\n",
    "         'eta': 0.1, # not tuned, learning rate with default of 0.3\n",
    "         'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "         'alpha': 2.6456,\n",
    "         'gamma': 6.4589, \n",
    "         'subsample': 0.9893,\n",
    "         'colsample_bytree': 0.6759,\n",
    "         'min_child_weight': 16,\n",
    "         'max_delta_step': 0\n",
    "        }\n",
    "\n",
    "num_round = 5000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "    return True\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n",
    "\n",
    "        \n",
    "def output_model_metrics( x, ypred, y_known, t ):\n",
    "    #Print model report:\n",
    "    mae = metrics.mean_absolute_error(y_known, ypred)\n",
    "    r2 = metrics.explained_variance_score(y_known, ypred)\n",
    "  \n",
    "    print(\"Model Report:\\t%s \\t n:\\t%i \\t\\t MAE Score:\\t%f \\t\\t R^2:\\t%f\" % (t, len(y_known), mae, r2))\n",
    "\n",
    "    \n",
    "def train_model(train, test, factors, label, xgb_model = None):\n",
    "    dtrain = xgb.DMatrix(train[factors].values, label=train[label], feature_names=factors)\n",
    "    dtest = xgb.DMatrix(test[factors].values, label=test[label], feature_names=factors)\n",
    "    watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "    progress = dict()\n",
    "  \n",
    "    xgb_model = xgb.train( param, dtrain, num_round, evals = watchlist, xgb_model = xgb_model, evals_result=progress\n",
    "                        , early_stopping_rounds = 10, verbose_eval = 20 )\n",
    "        \n",
    "    if hasattr(xgb_model, 'best_score'):\n",
    "        slack(\"Early stopping occured, best_score %f, best_iteration %i\" % (xgb_model.best_score, xgb_model.best_iteration))\n",
    "\n",
    "    curve = pd.DataFrame()\n",
    "    curve['test'] = progress['eval']['mae']\n",
    "    curve['train'] = progress['train']['mae']\n",
    "\n",
    "    url = plot_rounds(curve.plot())\n",
    "    slack(\"\", url, \"MAE by Round ($)\")\n",
    "    \n",
    "    url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "    slack(\"\", url, \"Feature Importance (n trees)\")\n",
    "        \n",
    "    # predict the training set using the model - note this is in sample testing\n",
    "    ypred = xgb_model.predict(dtrain, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtrain, ypred, train[label], 'train' )\n",
    "\n",
    "    # predict the testing set using the model\n",
    "    ypred = xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtest, ypred, test[label], 'test' )\n",
    "    \n",
    "    # clean out the model from memory\n",
    "    xgb_model.save_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "    del xgb_model\n",
    "    gc.collect()    \n",
    "\n",
    "def queue_reads():\n",
    "    # read in all of the files in the same order we ran queries\n",
    "    sales = pd.read_csv('CSV_backups/ALL-sales.csv',nrows=limit, index_col=['property_id','transaction_id'])\n",
    "    for_sale = pd.read_csv('CSV_backups/ALL-for_sale.csv',nrows=limit, index_col=['property_id','transaction_id'])\n",
    "    \n",
    "    sales_train, sales_test = cv.train_test_split(sales, test_size = 0.2) # set aside X% of the dataset for testing\n",
    "    del sales\n",
    "\n",
    "    return sales_train, sales_test, for_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (12,13,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/home/ilya/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (12,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "city = 'ALL'\n",
    "limit = 10000000\n",
    "\n",
    "sales_train, sales_test, for_sale = queue_reads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_closed</th>\n",
       "      <th>date_listed</th>\n",
       "      <th>days_on_market</th>\n",
       "      <th>transaction_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price, price_closed, date_listed, days_on_market, transaction_status]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_sale['date_listed'] = pd.to_datetime(for_sale.date_listed)\n",
    "\n",
    "for_sale['days_on_market'] = (today - for_sale.date_listed).apply(lambda x: x.days)\n",
    "for_sale = for_sale[for_sale.days_on_market < 180]\n",
    "\n",
    "for_sale[['price','price_closed','date_listed','days_on_market', 'transaction_status']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "limit = min(limit, len(sales_train.index))\n",
    "    \n",
    "ind2remove = ['Unnamed: 0', 'address', 'area_name', 'date_listed', 'id', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images', 'date_transacted_latest','school_district_id','zestimate_sale','zestimate_rent']\n",
    "factors = np.setdiff1d(sales_train.columns, ind2remove).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'PH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b9cd64ee3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msales_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mmemory_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# split the dataset into 250k chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msales_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# load the model into memory - should have been saved by train_model function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee050da6395a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train, test, factors, label, xgb_model)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mwatchlist\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/Code/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/Code/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# and we explicitly tell np.array to try and avoid copying)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'PH'"
     ]
    }
   ],
   "source": [
    "# cap number of homes that fit into VRAM\n",
    "memory_cap = 250000\n",
    "\n",
    "# init empty model that we can load into on the second iteration\n",
    "bst = xgb.Booster()\n",
    "\n",
    "# first run the price model\n",
    "label = 'price'\n",
    "f = factors\n",
    "f.remove(label) # this happens in place\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for g, df in sales_train.groupby(np.arange(len(sales_train.index)) // memory_cap):  # split the dataset into 250k chunks    \n",
    "    train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "\n",
    "    # load the model into memory - should have been saved by train_model function\n",
    "    bst.load_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "ypred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))\n",
    "\n",
    "# second run the days on the market model\n",
    "sales_train = sales_train[(sales_train.days_on_market > 0 )]\n",
    "sales_test = sales_test[(sales_test.days_on_market > 0 )]\n",
    "label = 'days_on_market'\n",
    "f = factors\n",
    "f.remove(label)\n",
    "\n",
    "for g, df in sales_train.groupby(np.arange(len(sales_train)) // memory_cap):  # split the dataset into 100k chunks    \n",
    "    train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "\n",
    "    # load the model into memory - should have been saved by train_model function\n",
    "    bst.load_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "dayspred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values = np.column_stack((for_sale.index.get_level_values(0)\n",
    "                         ,for_sale.index.get_level_values(1)\n",
    "                         ,for_sale.address.values\n",
    "                         ,for_sale.zipcode.values\n",
    "                         ,ypred-for_sale.price\n",
    "                         ,ypred\n",
    "                         ,for_sale.price.values\n",
    "                         ,for_sale['origin_url'].values))\n",
    "index = pd.MultiIndex.from_tuples(for_sale.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['address', 'zipcode', 'ypred', 'predicted_price', 'list', 'url'])\n",
    "output = output.sort_values(by='ypred',ascending=False)\n",
    "\n",
    "\n",
    "# save target list\n",
    "file = csv_path + 'value_buy/vb_target_list_' + today.strftime('%Y%m%d') + '.csv'\n",
    "output.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sales_test.index)\n",
    "sales_test['date_closed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_test['year'] = sales_test['date_closed'].apply(lambda x: (dt.timedelta(days=x) + dt.date(2000,1,1)).year)\n",
    "sales_test[['year','date_closed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(sales_test[factors].values, label=sales_test[label], feature_names=factors)\n",
    "ypred = bst.predict(dtest, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))\n",
    "\n",
    "\n",
    "values = np.column_stack((sales_test.index.get_level_values(0)\n",
    "                         ,sales_test.index.get_level_values(1)\n",
    "                         ,sales_test.year\n",
    "                         ,np.absolute(ypred-sales_test.price)\n",
    "                         ,ypred\n",
    "                         ,sales_test.price.values))\n",
    "index = pd.MultiIndex.from_tuples(sales_test.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['year', 'error', 'predicted_price', 'sale price'])\n",
    "output = output.sort_values(by='error',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yrs = output[(output.year > 2000) & (output.year < 2018)].groupby(['year'])\n",
    "yrs['error'].mean().plot.bar(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.error.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(output.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(yrs.get_group(2017)['sale price'] - yrs.get_group(2017).predicted_price).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('CSV_backups/ALL-sales.csv',nrows=limit, index_col=['property_id','transaction_id'])\n",
    "sales = sales[sales.date_closed > (6413-31)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_id = 256927\n",
    "sales.loc[sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst.load_model(model_path +  'all_' + label + '_' + '20170617' + '.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = xgb.DMatrix(sales[factors].values, label=sales[label], feature_names=factors)\n",
    "ypred = bst.predict(sample, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = np.column_stack((sales.index.get_level_values(0)\n",
    "                         ,sales.index.get_level_values(1)\n",
    "                         ,np.absolute(ypred-sales.price)\n",
    "                         ,ypred\n",
    "                         ,sales.price.values))\n",
    "index = pd.MultiIndex.from_tuples(sales.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['error', 'predicted_price', 'sale price'])\n",
    "output = output.sort_values(by='error',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(today - dt.date(2000,1,1)).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.error.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
