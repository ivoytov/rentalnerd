{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "# %sql mysql://root@localhost/rental_nerd\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = (dt.date.today() - dt.date(2000, 1, 1)).days\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "model_path = '/home/ilya/rentalnerd-models/'\n",
    "csv_path = '/home/ilya/Code/rentalnerd/scraper/'\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'binary:logistic',\n",
    "#         'booster': 'gbtree',\n",
    "         'eval_metric':'error', \n",
    "#         'tree_method': 'gpu_exact',\n",
    "#         'n_gpus': 1,\n",
    "#         'gpu_id': 0\n",
    "#         'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "#         'alpha': 2.6456,\n",
    "#         'gamma': 6.4589, \n",
    "#         'subsample': 0.9893,\n",
    "#         'colsample_bytree': 0.6759,\n",
    "#         'min_child_weight': 16,\n",
    "#         'max_delta_step': 0\n",
    "#          'max_depth': 15,  # tuning from 6/9/2017\n",
    "#          'alpha': 8.97,\n",
    "#          'gamma': 9.37, \n",
    "#          'subsample': 0.8,\n",
    "#          'colsample_bytree': 0.35,\n",
    "#          'min_child_weight': 20,\n",
    "#          'max_delta_step': 0.09\n",
    "#          'max_depth': 5,  # tuning from 6/10/2017\n",
    "#          'alpha': 0,\n",
    "#          'gamma': 20, \n",
    "#          'subsample': 1,\n",
    "#          'colsample_bytree': 1,\n",
    "#          'min_child_weight': 1,\n",
    "#          'max_delta_step': 9.6\n",
    "# tuning from 9/30/2017\n",
    "           'eta': 0.0362,\n",
    "           'gamma': 0.1722,\n",
    "           'max_delta_step': 9.6907,\n",
    "           'max_depth': 7,\n",
    "           'min_child_weight': 19.8647\n",
    "        }\n",
    "\n",
    "num_round = 5000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))\n",
    "\n",
    "discount = 0.050 # consider sales within this percent of list to be \"good sales\"\n",
    "if discount > 1:\n",
    "    slack(\"ALERT DISCOUNT MUST BE LESS THAN 100%\")\n",
    "    \n",
    "    \n",
    "iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 100000000\n",
    "price_cap = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (12,13,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sold: 512215 and for_sale: 958\n"
     ]
    }
   ],
   "source": [
    "# read in all of the files in the same order we ran queries\n",
    "sold = pd.read_csv('CSV_backups/ALL-sales.csv',nrows=limit, index_col=['property_id','transaction_id']).drop_duplicates()\n",
    "for_sale = pd.read_csv('CSV_backups/ALL-for_sale.csv',nrows=limit, index_col=['property_id','transaction_id']) \\\n",
    "             .drop_duplicates()\n",
    "             \n",
    "for_sale = for_sale[for_sale.price < price_cap]\n",
    "        \n",
    "print(\"Length of sold: %i and for_sale: %i\" % (len(sold.index), len(for_sale.index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_sale['days_on_market'] = today - for_sale.date_listed\n",
    "sold['any_sell'] = (sold.transaction_status == 'closed')\n",
    "\n",
    "# for_sale[['price','price_closed','date_listed','days_on_market', 'transaction_status']].tail(20)sold['good_sell'] = (sold.price_closed >= (sold.price_listed * (1 - discount )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff62d6ca6d8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff62def3ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for_sale.days_on_market.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(sold, f, label):\n",
    "    sales_train, sales_test = cv.train_test_split(sold, test_size = 0.25) # set aside X% of the dataset for testing\n",
    "\n",
    "    dtrain = xgb.DMatrix(sales_train[f], label=sales_train[label], feature_names=sales_train[f].columns.values)   \n",
    "    dtest = xgb.DMatrix(sales_test[f], label=sales_test[label], feature_names=f)\n",
    "    watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "\n",
    "    progress = dict()\n",
    "    xgb_model = xgb.train( param, dtrain, num_round, evals = watchlist, early_stopping_rounds = 100 \n",
    "                          , verbose_eval = 50, evals_result = progress )\n",
    "\n",
    "    if hasattr(xgb_model, 'best_score'):\n",
    "        print(\"Early stopping occured, best_score %f, best_iteration %i\" % (xgb_model.best_score, xgb_model.best_iteration))\n",
    "    \n",
    "    return xgb_model,progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind2remove = ['Unnamed: 0', 'id', 'address', 'area_name', 'date_listed', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images','zestimate_sale','zestimate_rent', 'price', 'price_closed', 'date_transacted_latest', \n",
    "              'school_district_id', 'broker_phone','broker_name','broker_license', 'broker_company', 'recrawled_at', 'city_code']\n",
    "factors = np.setdiff1d(sold.columns, ind2remove).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sales_train, sales_test = cv.train_test_split(sold, test_size = 0.25) # set aside X% of the dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model iteration 1\n",
      "[0]\ttrain-error:0.085569\teval-error:0.092683\n",
      "Multiple eval metrics have been passed: 'eval-error' will be used for early stopping.\n",
      "\n",
      "Will train until eval-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.077439\teval-error:0.086585\n",
      "[100]\ttrain-error:0.068293\teval-error:0.090854\n",
      "Stopping. Best iteration:\n",
      "[25]\ttrain-error:0.080691\teval-error:0.082317\n",
      "\n",
      "Early stopping occured, best_score 0.082317, best_iteration 25\n",
      "Running model iteration 2\n",
      "[0]\ttrain-error:0.081098\teval-error:0.094512\n",
      "Multiple eval metrics have been passed: 'eval-error' will be used for early stopping.\n",
      "\n",
      "Will train until eval-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.073374\teval-error:0.089634\n",
      "[100]\ttrain-error:0.066057\teval-error:0.085976\n",
      "[150]\ttrain-error:0.05935\teval-error:0.089024\n",
      "Stopping. Best iteration:\n",
      "[67]\ttrain-error:0.069309\teval-error:0.082927\n",
      "\n",
      "Early stopping occured, best_score 0.082927, best_iteration 67\n",
      "Running model iteration 3\n",
      "[0]\ttrain-error:0.081504\teval-error:0.090244\n",
      "Multiple eval metrics have been passed: 'eval-error' will be used for early stopping.\n",
      "\n",
      "Will train until eval-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.07561\teval-error:0.089024\n",
      "[100]\ttrain-error:0.067073\teval-error:0.090244\n",
      "Stopping. Best iteration:\n",
      "[18]\ttrain-error:0.078862\teval-error:0.086585\n",
      "\n",
      "Early stopping occured, best_score 0.086585, best_iteration 18\n",
      "Running model iteration 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f5dbde4628fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running model iteration %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msold_recently\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# load for sale properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-128a4e45ca0d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(sold, f, label)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msales_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msales_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msales_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mwatchlist\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/Code/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/Code/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# and we explicitly tell np.array to try and avoid copying)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the probability of the home being sold at all (good or bad sell)\n",
    "\n",
    "label = 'any_sell'\n",
    "sold_recently = sold[sold.date_listed > (today - 90)]\n",
    "\n",
    "ind2remove = ['Unnamed: 0', 'id', 'address', 'area_name', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images','zestimate_sale','zestimate_rent', 'price', 'date_closed', 'price_closed', 'date_transacted_latest', \n",
    "              'school_district_id', 'broker_phone','broker_name','broker_license', 'broker_company', 'recrawled_at', \n",
    "              'city_code']\n",
    "factors = np.setdiff1d(sold.columns, ind2remove).tolist()\n",
    "\n",
    "f = factors  # copy to a new array, in case we want to rerun any cells above\n",
    "f.remove(label) # this happens in place\n",
    "\n",
    "ypred = np.empty([for_sale.shape[0],iterations])\n",
    "\n",
    "for x in range(iterations):\n",
    "    print(\"Running model iteration %i\" % (x+1))\n",
    "        \n",
    "    xgb_model,progress = train_model(sold_recently, f, label)\n",
    "    \n",
    "    # load for sale properties\n",
    "    target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "    ypred[:,x] = xgb_model.predict(target, ntree_limit=(xgb_model.best_iteration if hasattr(xgb_model, 'best_score') else None))\n",
    "    \n",
    "    \n",
    "# save the average ypred value across all iterations\n",
    "as_ypred_mean = ypred.mean(axis=1)\n",
    "print(\"average prediction = %f\" % as_ypred_mean.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the last iterated model (not necessarily the best, but whatever)\n",
    "xgb_model.save_model(model_path + 'any_sell_' + dt.date.today().strftime('%Y%m%d') + '.model')\n",
    "\n",
    "# output the graph of accuracy by model iteration, but only for the last run of the model\n",
    "curve = pd.DataFrame()\n",
    "curve['test'] = progress['eval']['error']\n",
    "curve['train'] = progress['train']['error']\n",
    "\n",
    "url = plot_rounds(curve.plot())\n",
    "slack(\"\", url, \"Error by Round (%)\")\n",
    "\n",
    "url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "slack(\"\", url, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the Probability of Good Sell\n",
    "\n",
    "sold = sold[sold.transaction_status == 'closed']\n",
    "sold['good_sell'] = (sold.price_closed >= (sold.price_listed * (1 - discount )))\n",
    "\n",
    "label = 'good_sell'\n",
    "\n",
    "f = factors  # copy to a new array, in case we want to rerun any cells above\n",
    "\n",
    "ypred = np.empty([for_sale.shape[0],iterations])\n",
    "\n",
    "for x in range(iterations):\n",
    "    print(\"Running model iteration %i\" % (x+1))\n",
    "        \n",
    "    xgb_model,progress = train_model(sold, f, label)\n",
    "    \n",
    "    # load for sale properties\n",
    "    target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "    ypred[:,x] = xgb_model.predict(target, ntree_limit=(xgb_model.best_iteration if hasattr(xgb_model, 'best_score') else None))\n",
    "    \n",
    "    \n",
    "# save the average ypred value across all iterations\n",
    "gs_ypred_mean = ypred.mean(axis=1)\n",
    "print(\"average prediction = %f\" % gs_ypred_mean.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the last iterated model (not necessarily the best, but whatever)\n",
    "xgb_model.save_model(model_path + 'good_sell_' + dt.date.today().strftime('%Y%m%d') + '.model')\n",
    "xgb_model.save_model('../service/models' + 'good_sell_' + dt.date.today().strftime('%Y%m%d') + '.model')\n",
    "\n",
    "\n",
    "# output the graph of accuracy by model iteration, but only for the last run of the model\n",
    "curve = pd.DataFrame()\n",
    "curve['test'] = progress['eval']['error']\n",
    "curve['train'] = progress['train']['error']\n",
    "\n",
    "url = plot_rounds(curve.plot())\n",
    "slack(\"\", url, \"Error by Round (%)\")\n",
    "\n",
    "url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "slack(\"\", url, \"Feature Importance\")\n",
    "\n",
    "values = np.column_stack((for_sale.index.get_level_values(0)\n",
    "                         ,for_sale.index.get_level_values(1)\n",
    "                         ,for_sale.address.values\n",
    "                         ,for_sale.zipcode.values\n",
    "                         ,as_ypred_mean\n",
    "                         ,gs_ypred_mean\n",
    "                         ,gs_ypred_mean * as_ypred_mean\n",
    "                         ,for_sale.price.values\n",
    "                         ,for_sale['origin_url'].values\n",
    "                         ,for_sale.latitude.values\n",
    "                         ,for_sale.longitude.values\n",
    "                         ,for_sale.date_listed.values\n",
    "                         ,for_sale.date_transacted_latest.values))\n",
    "index = pd.MultiIndex.from_tuples(for_sale.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['address', 'zipcode','any_sell', 'good_sell','bayesian', 'list', 'url', 'lat', 'long', 'date_listed', 'transaction_date']) \\\n",
    "            .sort_values(by='bayesian',ascending=False) \\\n",
    "            .drop_duplicates()\n",
    "\n",
    "file = csv_path + 'good_sell/gs_target_list_' + dt.date.today().strftime('%Y%m%d') + '.csv'\n",
    "output.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load for sale properties\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "ypred = xgb_model.predict(target, ntree_limit=(xgb_model.best_iteration if hasattr(xgb_model, 'best_score') else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_id = 464422\n",
    "#df = for_sale.loc[my_id]\n",
    "df = pd.read_csv('Scripts/service_df.csv', index_col=['property_id'])\n",
    "\n",
    "my_target = xgb.DMatrix( df[f].values, feature_names=f)\n",
    "\n",
    "print(factors)\n",
    "import csv\n",
    "\n",
    "with open('Scripts/factors.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(factors)\n",
    "    \n",
    "\n",
    "with open('Scripts/factors.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    read_factors = list(reader)\n",
    "\n",
    "read_factors = read_factors[0]\n",
    "print(read_factors)\n",
    "\n",
    "f = np.array(['adult', 'bathrooms', 'bedrooms', 'city_code_PH', 'construction', 'date_listed', 'days_on_market', 'days_on_market_accu', 'dist_to_airport', 'dist_to_golf_course', 'dist_to_hiway', 'dist_to_lightrail_line', 'dist_to_lightrail_station', 'dist_to_park', 'dist_to_railway', 'dist_to_shopping', 'dist_to_waterway', 'elevation', 'fixer', 'foreclosure', 'fsbo', 'price_listed','garage', 'has_pool', 'hoa_fees', 'is_latest', 'latitude', 'level', 'longitude', 'lot', 'mobile', 'near_golf_course', 'rooms', 'saves', 'school_district_id_0.0', 'school_district_id_100.0', 'school_district_id_11.0', 'school_district_id_124.0', 'school_district_id_162.0', 'school_district_id_168.0', 'school_district_id_172.0', 'school_district_id_187.0', 'school_district_id_19.0', 'school_district_id_190.0', 'school_district_id_222.0', 'school_district_id_224.0', 'school_district_id_28.0', 'school_district_id_35.0', 'school_district_id_37.0', 'school_district_id_40.0', 'school_district_id_43.0', 'school_district_id_47.0', 'school_district_id_48.0', 'school_district_id_5.0', 'school_district_id_57.0', 'school_district_id_60.0', 'school_district_id_67.0', 'school_district_id_68.0', 'school_district_id_75.0', 'school_district_id_87.0', 'school_district_id_90.0', 'school_district_id_93.0', 'school_district_id_96.0', 'sqft', 'stories', 'townhouse', 'year_built', 'zipcode_85003', 'zipcode_85004', 'zipcode_85006', 'zipcode_85007', 'zipcode_85008', 'zipcode_85009', 'zipcode_85012', 'zipcode_85013', 'zipcode_85014', 'zipcode_85015', 'zipcode_85016', 'zipcode_85017', 'zipcode_85018', 'zipcode_85019', 'zipcode_85020', 'zipcode_85021', 'zipcode_85022', 'zipcode_85023', 'zipcode_85024', 'zipcode_85027', 'zipcode_85028', 'zipcode_85029', 'zipcode_85031', 'zipcode_85032', 'zipcode_85033', 'zipcode_85034', 'zipcode_85035', 'zipcode_85037', 'zipcode_85042', 'zipcode_85043', 'zipcode_85044', 'zipcode_85050', 'zipcode_85051', 'zipcode_85053', 'zipcode_85085', 'zipcode_85087', 'zipcode_85138', 'zipcode_85139', 'zipcode_85201', 'zipcode_85202', 'zipcode_85203', 'zipcode_85205', 'zipcode_85206', 'zipcode_85207', 'zipcode_85209', 'zipcode_85210', 'zipcode_85212', 'zipcode_85215', 'zipcode_85224', 'zipcode_85225', 'zipcode_85226', 'zipcode_85249', 'zipcode_85250', 'zipcode_85251', 'zipcode_85253', 'zipcode_85254', 'zipcode_85255', 'zipcode_85257', 'zipcode_85258', 'zipcode_85259', 'zipcode_85260', 'zipcode_85283', 'zipcode_85284', 'zipcode_85286', 'zipcode_85301', 'zipcode_85302', 'zipcode_85303', 'zipcode_85304', 'zipcode_85305', 'zipcode_85306', 'zipcode_85307', 'zipcode_85308', 'zipcode_85310', 'zipcode_85345', 'zipcode_85351', 'zipcode_85353', 'zipcode_85373', 'zipcode_85381', 'zipcode_85382', 'zipcode_85383', 'zipcode_85396', 'zipcode_85936']).tolist()\n",
    "\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(model_path + 'good_sell_20180121.model')\n",
    "\n",
    "def my_range(start, end, step):\n",
    "    while start <= end:\n",
    "        yield start\n",
    "        start += step\n",
    "\n",
    "\n",
    "        \n",
    "print(f)\n",
    "print(df[f].values)\n",
    "output = \"\"\n",
    "for x in my_range(100000, 200000, 10000):\n",
    "    df['price_listed'] = x\n",
    "\n",
    "    target = xgb.DMatrix( df[read_factors], feature_names=(read_factors))\n",
    "    ypred = bst.predict(target, ntree_limit=int(bst.attributes()['best_iteration']))\n",
    "\n",
    "    output = output + (\"Predicted good sell at price $%i: %f\\n\" % (df.price_listed.iloc[0], ypred))\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output.any_sell = output.any_sell.astype('float')\n",
    "# output.good_sell = output.good_sell.astype('float')\n",
    "# output.plot.scatter(x='any_sell', y='good_sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural network code\n",
    "label = 'good_sell'\n",
    "\n",
    "ind2remove = ['Unnamed: 0', 'id', 'address', 'area_name', 'date_listed', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images','zestimate_sale','zestimate_rent', 'price', 'price_closed', 'date_transacted_latest', \n",
    "              'school_district_id', 'broker_phone','broker_name','broker_license', 'broker_company', 'recrawled_at', 'city_code']\n",
    "factors = np.setdiff1d(sold.columns, ind2remove).tolist()\n",
    "\n",
    "# simple list of factors to start\n",
    "zipcode_factors = [x for x in factors if not x.find('zipcode')]\n",
    "school_district_factors = [x for x in factors if not x.find('school_district_id')]\n",
    "city_factors = [x for x in factors if not x.find('city_code')]\n",
    "factors = [label,'bathrooms', 'bedrooms', 'date_closed','sqft', 'date_closed', 'longitude', 'latitude', 'garage', \\\n",
    "           'near_golf_course','has_pool','level', 'adult', 'construction', 'townhouse', 'mobile'] \\\n",
    "            + zipcode_factors + school_district_factors + city_factors\n",
    "\n",
    "f = factors  # copy to a new array, in case we want to rerun any cells above\n",
    "f.remove(label) # this happens in place\n",
    "\n",
    "sales_train, sales_test = cv.train_test_split(sold, test_size = 0.25) # set aside X% of the dataset for testing\n",
    "X_train = sales_train[f]\n",
    "y_train = sales_train[label]\n",
    "X_test = sales_test[f]\n",
    "y_test = sales_test[label]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001, solver='sgd', verbose=10, tol=0.000000001)\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict the current for sale universe\n",
    "ypred = mlp.predict_proba(scaler.transform(for_sale[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = np.column_stack((for_sale.index.get_level_values(0)\n",
    "                         ,for_sale.index.get_level_values(1)\n",
    "                         ,for_sale.address.values\n",
    "                         ,for_sale.zipcode.values\n",
    "                         ,ypred[:,1]\n",
    "                         ,for_sale.price.values\n",
    "                         ,for_sale['origin_url'].values\n",
    "                         ,for_sale.latitude.values\n",
    "                         ,for_sale.longitude.values\n",
    "                         ,for_sale.date_listed.apply(lambda x: x.strftime('%Y-%m-%d')).values\n",
    "                         ,for_sale.date_transacted_latest.values))\n",
    "index = pd.MultiIndex.from_tuples(for_sale.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['address', 'zipcode','ypred', 'list', 'url', 'lat', 'long', 'date_listed', 'transaction_date']) \\\n",
    "            .sort_values(by='ypred',ascending=False) \\\n",
    "            .drop_duplicates()\n",
    "\n",
    "file = csv_path + 'neural_network/nn_target_list_' + today.strftime('%Y%m%d') + '.csv'\n",
    "output.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind2remove = ['Unnamed: 0', 'id', 'address', 'area_name', 'date_listed', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images','zestimate_sale','zestimate_rent', 'price', 'price_closed', 'date_transacted_latest', \n",
    "              'school_district_id', 'broker_phone','broker_name','broker_license', 'broker_company', 'recrawled_at', 'city_code']\n",
    "all_factors = np.setdiff1d(sold.columns, ind2remove).tolist()\n",
    "\n",
    "diff(all_factors,factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp.coefs_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
