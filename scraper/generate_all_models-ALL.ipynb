{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "# %sql mysql://root@localhost/rental_nerd\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "    \n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = dt.date.today()\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "model_path = '/home/ilya/rentalnerd-models/'\n",
    "csv_path = '/home/ilya/Code/rentalnerd/scraper/'\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'reg:linear',\n",
    "         'booster': 'gbtree',\n",
    "         'eval_metric':'mae', \n",
    "#          'updater': 'grow_gpu',\n",
    "         'eta': 0.1, # not tuned, learning rate with default of 0.3\n",
    "         'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "         'alpha': 2.6456,\n",
    "         'gamma': 6.4589, \n",
    "         'subsample': 0.9893,\n",
    "         'colsample_bytree': 0.6759,\n",
    "         'min_child_weight': 16,\n",
    "         'max_delta_step': 0\n",
    "        }\n",
    "\n",
    "num_round = 5000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n",
    "\n",
    "        \n",
    "def output_model_metrics( x, ypred, y_known, t ):\n",
    "    #Print model report:\n",
    "    mae = metrics.mean_absolute_error(y_known, ypred)\n",
    "    r2 = metrics.explained_variance_score(y_known, ypred)\n",
    "  \n",
    "    slack(\"Model Report:\\t%s \\t n:\\t%i \\t\\t MAE Score:\\t%f \\t\\t R^2:\\t%f\" % (t, len(y_known), mae, r2))\n",
    "\n",
    "    \n",
    "def train_model(train, test, factors, label, xgb_model = None):\n",
    "    dtrain = xgb.DMatrix(train[factors].values, label=train[label], feature_names=factors)\n",
    "    dtest = xgb.DMatrix(test[factors].values, label=test[label], feature_names=factors)\n",
    "    watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "    progress = dict()\n",
    "  \n",
    "    xgb_model = xgb.train( param, dtrain, num_round, evals = watchlist, xgb_model = xgb_model, evals_result=progress\n",
    "                        , early_stopping_rounds = 10, verbose_eval = 20 )\n",
    "        \n",
    "    if hasattr(xgb_model, 'best_score'):\n",
    "        slack(\"Early stopping occured, best_score %f, best_iteration %i\" % (xgb_model.best_score, xgb_model.best_iteration))\n",
    "\n",
    "    curve = pd.DataFrame()\n",
    "    curve['test'] = progress['eval']['mae']\n",
    "    curve['train'] = progress['train']['mae']\n",
    "\n",
    "    url = plot_rounds(curve.plot())\n",
    "    slack(\"\", url, \"MAE by Round ($)\")\n",
    "    \n",
    "    url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "    slack(\"\", url, \"Feature Importance (n trees)\")\n",
    "        \n",
    "    # predict the training set using the model - note this is in sample testing\n",
    "    ypred = xgb_model.predict(dtrain, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtrain, ypred, train[label], 'train' )\n",
    "\n",
    "    # predict the testing set using the model\n",
    "    ypred = xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtest, ypred, test[label], 'test' )\n",
    "    \n",
    "    # clean out the model from memory\n",
    "    xgb_model.save_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "    del xgb_model\n",
    "    gc.collect()    \n",
    "\n",
    "def queue_reads():\n",
    "    # read in all of the files in the same order we ran queries\n",
    "    sales = pd.read_csv('CSV_backups/ALL-sales.csv',nrows=limit, index_col=['property_id','transaction_id'])\n",
    "    for_sale = pd.read_csv('CSV_backups/ALL-for_sale.csv',nrows=limit, index_col=['property_id','transaction_id'])\n",
    "    \n",
    "    sales_train, sales_test = cv.train_test_split(sales, test_size = 0.2) # set aside X% of the dataset for testing\n",
    "    del sales\n",
    "\n",
    "    return sales_train, sales_test, for_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city = 'ALL'\n",
    "limit = 10000000\n",
    "\n",
    "sales_train, sales_test, for_sale = queue_reads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_closed</th>\n",
       "      <th>date_listed</th>\n",
       "      <th>days_on_market</th>\n",
       "      <th>transaction_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7462170</th>\n",
       "      <th>13450217</th>\n",
       "      <td>242000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>113</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7459565</th>\n",
       "      <th>13441628</th>\n",
       "      <td>120000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>115</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7413372</th>\n",
       "      <th>13423254</th>\n",
       "      <td>382492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>122</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7437633</th>\n",
       "      <th>13371249</th>\n",
       "      <td>283654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>135</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427704</th>\n",
       "      <th>13361775</th>\n",
       "      <td>374145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>136</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7428452</th>\n",
       "      <th>13344675</th>\n",
       "      <td>225000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>152</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7405341</th>\n",
       "      <th>13303918</th>\n",
       "      <td>119000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>166</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399050</th>\n",
       "      <th>13303764</th>\n",
       "      <td>69900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>167</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7414060</th>\n",
       "      <th>13288806</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>153</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7413232</th>\n",
       "      <th>13286532</th>\n",
       "      <td>280140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-12</td>\n",
       "      <td>154</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946592</th>\n",
       "      <th>13281281</th>\n",
       "      <td>369700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>163</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022575</th>\n",
       "      <th>13277782</th>\n",
       "      <td>296713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>166</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404833</th>\n",
       "      <th>13242721</th>\n",
       "      <td>247505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>161</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399798</th>\n",
       "      <th>13242696</th>\n",
       "      <td>244580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>161</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399791</th>\n",
       "      <th>13242684</th>\n",
       "      <td>240750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>161</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7394811</th>\n",
       "      <th>13242679</th>\n",
       "      <td>248700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>161</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399742</th>\n",
       "      <th>13222508</th>\n",
       "      <td>74000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>173</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405286</th>\n",
       "      <th>13218203</th>\n",
       "      <td>299900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>171</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7397680</th>\n",
       "      <th>13214069</th>\n",
       "      <td>245990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>172</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387717</th>\n",
       "      <th>13175937</th>\n",
       "      <td>295000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>177</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             price  price_closed date_listed  days_on_market  \\\n",
       "property_id transaction_id                                                     \n",
       "7462170     13450217        242000           NaN  2017-03-25             113   \n",
       "7459565     13441628        120000           NaN  2017-03-23             115   \n",
       "7413372     13423254        382492           NaN  2017-03-16             122   \n",
       "7437633     13371249        283654           NaN  2017-03-03             135   \n",
       "7427704     13361775        374145           NaN  2017-03-02             136   \n",
       "7428452     13344675        225000           NaN  2017-02-14             152   \n",
       "7405341     13303918        119000           NaN  2017-01-31             166   \n",
       "7399050     13303764         69900           NaN  2017-01-30             167   \n",
       "7414060     13288806        200000           NaN  2017-02-13             153   \n",
       "7413232     13286532        280140           NaN  2017-02-12             154   \n",
       "1946592     13281281        369700           NaN  2017-02-03             163   \n",
       "2022575     13277782        296713           NaN  2017-01-31             166   \n",
       "7404833     13242721        247505           NaN  2017-02-05             161   \n",
       "7399798     13242696        244580           NaN  2017-02-05             161   \n",
       "7399791     13242684        240750           NaN  2017-02-05             161   \n",
       "7394811     13242679        248700           NaN  2017-02-05             161   \n",
       "7399742     13222508         74000           NaN  2017-01-24             173   \n",
       "1405286     13218203        299900           NaN  2017-01-26             171   \n",
       "7397680     13214069        245990           NaN  2017-01-25             172   \n",
       "7387717     13175937        295000           NaN  2017-01-20             177   \n",
       "\n",
       "                           transaction_status  \n",
       "property_id transaction_id                     \n",
       "7462170     13450217                     open  \n",
       "7459565     13441628                     open  \n",
       "7413372     13423254                     open  \n",
       "7437633     13371249                     open  \n",
       "7427704     13361775                     open  \n",
       "7428452     13344675                     open  \n",
       "7405341     13303918                     open  \n",
       "7399050     13303764                     open  \n",
       "7414060     13288806                     open  \n",
       "7413232     13286532                     open  \n",
       "1946592     13281281                     open  \n",
       "2022575     13277782                     open  \n",
       "7404833     13242721                     open  \n",
       "7399798     13242696                     open  \n",
       "7399791     13242684                     open  \n",
       "7394811     13242679                     open  \n",
       "7399742     13222508                     open  \n",
       "1405286     13218203                     open  \n",
       "7397680     13214069                     open  \n",
       "7387717     13175937                     open  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_sale['date_listed'] = pd.to_datetime(for_sale.date_listed)\n",
    "for_sale['days_on_market'] = (today - for_sale.date_listed).apply(lambda x: x.days)\n",
    "for_sale = for_sale[for_sale.days_on_market < 180]\n",
    "\n",
    "for_sale[['price','price_closed','date_listed','days_on_market', 'transaction_status']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "limit = min(limit, len(sales_train.index))\n",
    "    \n",
    "ind2remove = ['Unnamed: 0', 'address', 'area_name', 'date_listed', 'id', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images', 'date_transacted_latest','school_district_id','zestimate_sale','zestimate_rent']\n",
    "factors = np.setdiff1d(sales_train.columns, ind2remove).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:158974\teval-mae:158560\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 10 rounds.\n",
      "[20]\ttrain-mae:29143.7\teval-mae:29788.6\n",
      "[40]\ttrain-mae:19781\teval-mae:21111.3\n",
      "[60]\ttrain-mae:17976.4\teval-mae:19757.8\n",
      "[80]\ttrain-mae:17141.6\teval-mae:19249.9\n",
      "[100]\ttrain-mae:16665.6\teval-mae:18975.5\n",
      "[120]\ttrain-mae:16331.9\teval-mae:18813.6\n",
      "[140]\ttrain-mae:16057.1\teval-mae:18688.7\n",
      "[160]\ttrain-mae:15825.3\teval-mae:18577.8\n",
      "[180]\ttrain-mae:15561.8\teval-mae:18466.3\n",
      "[200]\ttrain-mae:15400.5\teval-mae:18403.1\n",
      "[220]\ttrain-mae:15216.2\teval-mae:18337.5\n"
     ]
    }
   ],
   "source": [
    "# cap number of homes that fit into VRAM\n",
    "memory_cap = 250000\n",
    "\n",
    "# init empty model that we can load into on the second iteration\n",
    "bst = xgb.Booster()\n",
    "\n",
    "# first run the price model\n",
    "label = 'price'\n",
    "f = factors\n",
    "f.remove(label) # this happens in place\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for g, df in sales_train.groupby(np.arange(len(sales_train.index)) // memory_cap):  # split the dataset into 250k chunks    \n",
    "    train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "\n",
    "    # load the model into memory - should have been saved by train_model function\n",
    "    bst.load_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "ypred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))\n",
    "\n",
    "# second run the days on the market model\n",
    "sales_train = sales_train[(sales_train.days_on_market > 0 )]\n",
    "sales_test = sales_test[(sales_test.days_on_market > 0 )]\n",
    "label = 'days_on_market'\n",
    "f = factors\n",
    "f.remove(label)\n",
    "\n",
    "for g, df in sales_train.groupby(np.arange(len(sales_train)) // memory_cap):  # split the dataset into 100k chunks    \n",
    "    train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "\n",
    "    # load the model into memory - should have been saved by train_model function\n",
    "    bst.load_model(model_path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "dayspred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values = np.column_stack((for_sale.index.get_level_values(0)\n",
    "                         ,for_sale.index.get_level_values(1)\n",
    "                         ,for_sale.address.values\n",
    "                         ,for_sale.zipcode.values\n",
    "                         ,ypred-for_sale.price\n",
    "                         ,ypred\n",
    "                         ,for_sale.price.values\n",
    "                         ,for_sale['origin_url'].values))\n",
    "index = pd.MultiIndex.from_tuples(for_sale.index.values, names=['property_id', 'transaction_id'])\n",
    "output = pd.DataFrame(values[:,2:],index=index,columns=['address', 'zipcode', 'ypred', 'predicted_price', 'list', 'url'])\n",
    "output = output.sort_values(by='ypred',ascending=False)\n",
    "\n",
    "\n",
    "# save target list\n",
    "file = csv_path + 'value_buy/vb_target_list_' + today.strftime('%Y%m%d') + '.csv'\n",
    "output.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
