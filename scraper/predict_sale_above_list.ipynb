{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "# %sql mysql://root@localhost/rental_nerd\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = dt.date.today()\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "model_path = '/home/ilya/rentalnerd-models/'\n",
    "csv_path = '/home/ilya/Code/rentalnerd/scraper/'\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'binary:logistic',\n",
    "         'booster': 'gbtree',\n",
    "         'eval_metric':'error', \n",
    "         'updater': 'grow_gpu',\n",
    "         'eta': 0.05, # not tuned, learning rate with default of 0.3\n",
    "#          'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "#          'alpha': 2.6456,\n",
    "#          'gamma': 6.4589, \n",
    "#          'subsample': 0.9893,\n",
    "#          'colsample_bytree': 0.6759,\n",
    "#          'min_child_weight': 16,\n",
    "#          'max_delta_step': 0\n",
    "         'max_depth': 15,  # all of the following parameters are __tuned__ so do not change them\n",
    "         'alpha': 8.97,\n",
    "         'gamma': 9.37, \n",
    "         'subsample': 0.8,\n",
    "         'colsample_bytree': 0.35,\n",
    "         'min_child_weight': 20,\n",
    "         'max_delta_step': 0.09\n",
    "        }\n",
    "\n",
    "num_round = 5000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))\n",
    "\n",
    "discount = 0.10 # consider sales within this percent of list to be \"good sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sanitize(data, zipcode_list = None):\n",
    "    # abort if the city has no top zipcodes\n",
    "    if data.empty:\n",
    "        return 0    \n",
    "    \n",
    "    data.drop(['abnormal', 'bookmarked', 'created_at', 'ignore', 'is_latest', 'id', 'closed_diff_id', 'listed_diff_id',\n",
    "                      'notes', 'source', 'updated_at', 'home_type', 'sfh', 'description', \n",
    "                    'event_name', 'neighborhood'], axis=1, inplace=True)\n",
    "    \n",
    "    # filters out any non-sensical values or fat finger mistakes in MLS listings\n",
    "    print(\"Entries before filter: \", len(data))\n",
    "\n",
    "    if(data.transaction_type.iloc[0] == 'sales'):\n",
    "        data = data[ data.price > 50000 ]\n",
    "    else:\n",
    "        data = data [ data.price > 500 ]\n",
    "    \n",
    "    if(zipcode_list is not None):\n",
    "        data = data[data.zipcode.isin(zipcode_list)]\n",
    "        \n",
    "#     data = data [ (data.price_closed - data.price_listed).abs() < 50000 ]\n",
    "\n",
    "    slack(\"Entries after filter: %i\" % len(data))\n",
    "    \n",
    "    # fills in some sensible defaults where data is missing\n",
    "    data[\"near_golf_course\"] = data[\"near_golf_course\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data[\"has_pool\"] = data[\"has_pool\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data[\"garage\"] = data[\"garage\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data['date_closed'] = data['date_closed'].apply(lambda x: 0 if x == None else (x - dt.date(2000, 1, 1)).days)\n",
    "    data['date_closed'] = data['date_closed'].astype(int)\n",
    "    \n",
    "    # convert the area name into dummy variables\n",
    "    dm = pd.get_dummies(data[['area_name', 'zipcode']], prefix=['area_name','zipcode'])\n",
    "    data = pd.concat([data, dm], axis=1)\n",
    "    del dm\n",
    "    \n",
    "    return data\n",
    "\n",
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = %sql (\\\n",
    "    SELECT \\\n",
    "    area_name_zipcodes.area_name, \\\n",
    "    properties.*, \\\n",
    "    property_transaction_logs.id as 'transaction_id', \\\n",
    "    property_transaction_logs.* \\\n",
    "    FROM  \\\n",
    "    property_transaction_logs \\\n",
    "    LEFT JOIN \\\n",
    "    properties on properties.id = property_transaction_logs.`property_id`  \\\n",
    "    LEFT JOIN \\\n",
    "    area_name_zipcodes on properties.zipcode = area_name_zipcodes.zipcode \\\n",
    "    where \\\n",
    "    home_type = 'sfh' AND \\\n",
    "    ( abnormal = false OR abnormal IS NULL OR abnormal = 0 ) and \\\n",
    "    properties.sqft between 1 and 10000 and \\\n",
    "    property_transaction_logs.price between 500 and 400000 and \\\n",
    "    properties.bedrooms <= 6 and \\\n",
    "    properties.bathrooms <= 6 and \\\n",
    "    transaction_type = 'sales' and  \\\n",
    "    date_closed is not null and \\\n",
    "    price_closed is not null and \\\n",
    "    days_on_market is not null and \\\n",
    "    transaction_status = 'closed' \\\n",
    "    ORDER BY property_transaction_logs.date_closed DESC \\\n",
    "    LIMIT :limit )\n",
    "\n",
    "\n",
    "closed = query.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = %sql (\\\n",
    "    SELECT \\\n",
    "    area_name_zipcodes.area_name, \\\n",
    "    properties.*, \\\n",
    "    property_transaction_logs.id as 'transaction_id', \\\n",
    "    property_transaction_logs.* \\\n",
    "    FROM  \\\n",
    "    property_transaction_logs \\\n",
    "    LEFT JOIN \\\n",
    "    properties on properties.id = property_transaction_logs.`property_id`  \\\n",
    "    LEFT JOIN \\\n",
    "    area_name_zipcodes on properties.zipcode = area_name_zipcodes.zipcode \\\n",
    "    where \\\n",
    "    home_type = 'sfh' AND \\\n",
    "    is_latest = 1 AND \\\n",
    "    ( abnormal = false OR abnormal IS NULL OR abnormal = 0 ) and \\\n",
    "    properties.sqft between 1 and 10000 and \\\n",
    "    property_transaction_logs.price between 500 and 400000 and \\\n",
    "    properties.bedrooms <= 6 and \\\n",
    "    properties.bathrooms <= 6 and \\\n",
    "    transaction_type = 'sales' AND \\\n",
    "    transaction_status = 'open' AND \\\n",
    "    area_name = 'PH' \\\n",
    "    LIMIT :limit )\n",
    "\n",
    "for_sale = query.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = pd.concat([closed,for_sale])\n",
    "q.set_index('property_id', inplace=True)\n",
    "q.index.name = 'property_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(closed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(q.columns)\n",
    "q = sanitize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_sale = q[q.transaction_status == 'open']\n",
    "sold = q[q.transaction_status == 'closed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sold['good_sell'] = (sold.price_closed >= (sold.price_listed * (1 - discount )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sold[['good_sell','price_listed','price_closed','days_on_market']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_sale['days_on_market'] = (today - for_sale.date_listed).apply(lambda x: x.days)\n",
    "\n",
    "for_sale[['price','price_closed','date_listed','days_on_market', 'transaction_status']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_sale.days_on_market.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind2remove = ['Unnamed: 0', 'id', 'address', 'area_name', 'date_listed', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images','zestimate_sale','zestimate_rent', 'price', 'price_closed', 'date_transacted_latest']\n",
    "factors = np.setdiff1d(sold.columns, ind2remove).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_train, sales_test = cv.train_test_split(sold, test_size = 0.25) # set aside X% of the dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cap number of homes that fit into VRAM\n",
    "memory_cap = 250000\n",
    "\n",
    "# init empty model that we can load into on the second iteration\n",
    "bst = xgb.Booster()\n",
    "\n",
    "# first run the price model\n",
    "label = 'good_sell'\n",
    "f = factors\n",
    "f.remove(label) # this happens in place\n",
    "\n",
    "dtrain = xgb.DMatrix(sales_train[f].values, label=sales_train[label], feature_names=sales_train[f].columns.values)\n",
    "dtest = xgb.DMatrix(sales_test[f].values, label=sales_test[label], feature_names=f)\n",
    "watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "\n",
    "progress = dict()\n",
    "xgb_model = xgb.train( param, dtrain, num_round, evals = watchlist, early_stopping_rounds = 50\n",
    "                      , verbose_eval = 10, evals_result = progress )\n",
    "\n",
    "if hasattr(xgb_model, 'best_score'):\n",
    "    print(\"Early stopping occured, best_score %f, best_iteration %i\" % (xgb_model.best_score, xgb_model.best_iteration))\n",
    "    \n",
    "slack(\"Training set: %i\\tTesting set: %i\" % (len(sales_train.index), len(sales_test.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model.save_model(model_path + 'good_sell_' + today.strftime('%Y%m%d') + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curve = pd.DataFrame()\n",
    "curve['test'] = progress['eval']['error']\n",
    "curve['train'] = progress['train']['error']\n",
    "\n",
    "url = plot_rounds(curve.plot())\n",
    "slack(\"\", url, \"Error by Round (%)\")\n",
    "\n",
    "url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "slack(\"\", url, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load for sale properties\n",
    "target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "ypred = xgb_model.predict(target, ntree_limit=(xgb_model.best_iteration if hasattr(xgb_model, 'best_score') else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values = np.column_stack((for_sale.index.values\n",
    "                         ,for_sale.transaction_id.values\n",
    "                         ,for_sale.address.values\n",
    "                         ,ypred\n",
    "                         ,for_sale.price.values\n",
    "                         ,for_sale['origin_url'].values))\n",
    "output = pd.DataFrame(values[:,1:],index=values[:,0],columns=['transaction_id', 'address','ypred',\n",
    "                                                              'list', 'url'])\n",
    "output.index.name = 'property_id'\n",
    "output = output.sort_values(by='ypred',ascending=False)\n",
    "# output = output[output.ypred > 0.50]\n",
    "file = csv_path + 'good_sell/target_list_' + today.strftime('%Y%m%d') + '.csv'\n",
    "output.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(for_sale.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
