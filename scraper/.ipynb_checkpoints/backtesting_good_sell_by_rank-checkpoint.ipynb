{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each models is required to generate a stack ranking of all properties listed for sale in the database.\n",
    "\n",
    "This script runs daily, reads in CSV files with a sorted order of homes (most attractive at the top to least attractive at the bottom), and a 'ypred' variable that reflects the scripts absolute metric (like P&L or probability of sale).\n",
    "\n",
    "The script compares each list with homes that were sold, and identifies closed transactions ('positives') as well as the top 10 properties that each model identified that didn't sell ('negatives'). The remainder is considered 'control set'. \n",
    "\n",
    "For the overall data set, and specifically for each of those 3 groups, we output the average 'ypred' value, what number and percentage of homes sold from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "csv_path = '/home/ilya/Code/rentalnerd/scraper/'\n",
    "today = dt.date.today()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "        \n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))\n",
    "\n",
    "# how old a target list to compare to the current target list\n",
    "lag_days = 40\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "discount = 0.050\n",
    "\n",
    "# to remove all the random far away areas - PHX only\n",
    "zipcode_list = ['85006', '85007', '85008', '85009', '85013', '85015', '85016', '85017',\n",
    "       '85018', '85019', '85020', '85021', '85022', '85023', '85024',\n",
    "       '85027', '85028', '85029', '85031', '85033', '85035', '85037',\n",
    "       '85040', '85041', '85043', '85044', '85048', '85050', '85051',\n",
    "       '85053', '85085', '85086', '85087', '85118', '85138', '85139',\n",
    "       '85142', '85143', '85202', '85203', '85204', '85205', '85206',\n",
    "       '85207', '85209', '85210', '85212', '85213', '85215', '85224',\n",
    "       '85225', '85226', '85233', '85234', '85248', '85249', '85250',\n",
    "       '85251', '85253', '85254', '85255', '85257', '85258', '85262',\n",
    "       '85266', '85286', '85295', '85296', '85297', '85298', '85301',\n",
    "       '85302', '85303', '85304', '85305', '85306', '85308', '85310',\n",
    "       '85323', '85326', '85331', '85335', '85339', '85340', '85351',\n",
    "       '85353', '85355', '85373', '85374', '85375', '85382', '85383',\n",
    "       '85388', '85390', '85392', '85396', '85704', '85705', '85706',\n",
    "       '85711', '85712']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (13,14,15,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "closed = pd.read_csv('CSV_backups/ALL-sales.csv',nrows=10000000, index_col=['property_id','transaction_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>transaction_status</th>\n",
       "      <th>date_listed</th>\n",
       "      <th>date_closed</th>\n",
       "      <th>days_on_market</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>is_latest</th>\n",
       "      <th>price_listed</th>\n",
       "      <th>price_closed</th>\n",
       "      <th>date_transacted_latest</th>\n",
       "      <th>...</th>\n",
       "      <th>school_district_id_57.0</th>\n",
       "      <th>school_district_id_60.0</th>\n",
       "      <th>school_district_id_67.0</th>\n",
       "      <th>school_district_id_68.0</th>\n",
       "      <th>school_district_id_75.0</th>\n",
       "      <th>school_district_id_87.0</th>\n",
       "      <th>school_district_id_90.0</th>\n",
       "      <th>school_district_id_93.0</th>\n",
       "      <th>school_district_id_96.0</th>\n",
       "      <th>school_district_id_nan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">9024826</th>\n",
       "      <th>23850293</th>\n",
       "      <td>128000</td>\n",
       "      <td>closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sales</td>\n",
       "      <td>False</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>128000</td>\n",
       "      <td>2011-05-20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23850291</th>\n",
       "      <td>131459</td>\n",
       "      <td>closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sales</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131459</td>\n",
       "      <td>2011-03-14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23850286</th>\n",
       "      <td>225000</td>\n",
       "      <td>closed</td>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>3114</td>\n",
       "      <td>105.0</td>\n",
       "      <td>sales</td>\n",
       "      <td>False</td>\n",
       "      <td>267500.0</td>\n",
       "      <td>225000</td>\n",
       "      <td>2008-07-11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23850285</th>\n",
       "      <td>118000</td>\n",
       "      <td>closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sales</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118000</td>\n",
       "      <td>1998-02-27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024825</th>\n",
       "      <th>23850282</th>\n",
       "      <td>105000</td>\n",
       "      <td>closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sales</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105000</td>\n",
       "      <td>2017-09-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             price transaction_status date_listed  \\\n",
       "property_id transaction_id                                          \n",
       "9024826     23850293        128000  closed             NaN          \n",
       "            23850291        131459  closed             NaN          \n",
       "            23850286        225000  closed             2008-03-28   \n",
       "            23850285        118000  closed             NaN          \n",
       "9024825     23850282        105000  closed             NaN          \n",
       "\n",
       "                            date_closed  days_on_market transaction_type  \\\n",
       "property_id transaction_id                                                 \n",
       "9024826     23850293        4157        NaN              sales             \n",
       "            23850291        4090        NaN              sales             \n",
       "            23850286        3114         105.0           sales             \n",
       "            23850285       -673         NaN              sales             \n",
       "9024825     23850282        6467        NaN              sales             \n",
       "\n",
       "                            is_latest  price_listed  price_closed  \\\n",
       "property_id transaction_id                                          \n",
       "9024826     23850293        False      125000.0      128000         \n",
       "            23850291        False     NaN            131459         \n",
       "            23850286        False      267500.0      225000         \n",
       "            23850285        False     NaN            118000         \n",
       "9024825     23850282        False     NaN            105000         \n",
       "\n",
       "                           date_transacted_latest          ...            \\\n",
       "property_id transaction_id                                 ...             \n",
       "9024826     23850293        2011-05-20                     ...             \n",
       "            23850291        2011-03-14                     ...             \n",
       "            23850286        2008-07-11                     ...             \n",
       "            23850285        1998-02-27                     ...             \n",
       "9024825     23850282        2017-09-15                     ...             \n",
       "\n",
       "                            school_district_id_57.0  school_district_id_60.0  \\\n",
       "property_id transaction_id                                                     \n",
       "9024826     23850293        0                        0                         \n",
       "            23850291        0                        0                         \n",
       "            23850286        0                        0                         \n",
       "            23850285        0                        0                         \n",
       "9024825     23850282        0                        0                         \n",
       "\n",
       "                           school_district_id_67.0 school_district_id_68.0  \\\n",
       "property_id transaction_id                                                   \n",
       "9024826     23850293        0                       0                        \n",
       "            23850291        0                       0                        \n",
       "            23850286        0                       0                        \n",
       "            23850285        0                       0                        \n",
       "9024825     23850282        0                       0                        \n",
       "\n",
       "                           school_district_id_75.0  school_district_id_87.0  \\\n",
       "property_id transaction_id                                                    \n",
       "9024826     23850293        0                       0                         \n",
       "            23850291        0                       0                         \n",
       "            23850286        0                       0                         \n",
       "            23850285        0                       0                         \n",
       "9024825     23850282        0                       0                         \n",
       "\n",
       "                           school_district_id_90.0 school_district_id_93.0  \\\n",
       "property_id transaction_id                                                   \n",
       "9024826     23850293        0                       0                        \n",
       "            23850291        0                       0                        \n",
       "            23850286        0                       0                        \n",
       "            23850285        0                       0                        \n",
       "9024825     23850282        0                       0                        \n",
       "\n",
       "                           school_district_id_96.0 school_district_id_nan  \n",
       "property_id transaction_id                                                 \n",
       "9024826     23850293        0                       0                      \n",
       "            23850291        0                       0                      \n",
       "            23850286        0                       0                      \n",
       "            23850285        0                       0                      \n",
       "9024825     23850282        0                       0                      \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag days 1\n",
      "\n",
      "\n",
      "Slacking: Prior target list length: 1206\tNum sold: 0\tAvg good sell prob: 0.528467\tAvg good sell prob of sold: nan\n",
      "Slacking: Num of homes with good_sell_prob > 0.622004: 302\tPerc of those sold: NONE\tMed disc to list: NA\n",
      "Slacking: Num of homes with good_sell_prob < 0.518153: 302\tPerc of those sold: NONE\tMedian disc to list: NA\n",
      "Slacking: Empty DataFrame\n",
      "Columns: [address, zipcode, good_sell_prob, list, url, lat, long, date_listed, transaction_date, price, good_sell]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# read in prior target list for backtesting purposes\n",
    "model_results = np.empty([0,7])\n",
    "for lag_days in range(1,2):\n",
    "    print(\"Lag days %i\\n\\n\" % lag_days)\n",
    "    try:\n",
    "        prior_target_list = pd.read_csv(csv_path + 'good_sell/gs_target_list_' + (today-dt.timedelta(days=lag_days)) \\\n",
    "                                    .strftime('%Y%m%d') + '.csv', index_col = [0,1]) \\\n",
    "                                    .rename(columns = { 'ypred' : 'good_sell_prob'})\n",
    "\n",
    "        backtest = prior_target_list.join(closed[['price']], how=\"inner\").sort_values(by='good_sell_prob',ascending=False)\n",
    "        backtest['good_sell'] = (backtest.price >= (backtest.list * (1-discount)))\n",
    "        y_all = prior_target_list.good_sell_prob.mean()\n",
    "        y_sold = backtest.good_sell_prob.mean()\n",
    "\n",
    "        pos_trigger = prior_target_list.good_sell_prob.quantile(0.75)\n",
    "        slack(\"Prior target list length: %i\\tNum sold: %i\\tAvg good sell prob: %f\\tAvg good sell prob of sold: %f\" \n",
    "              % (len(prior_target_list.index), len(backtest.index),y_all, y_sold))\n",
    "\n",
    "        num_pos = len(prior_target_list[prior_target_list.good_sell_prob > pos_trigger].index)\n",
    "        positives = backtest[backtest.good_sell_prob > pos_trigger]\n",
    "        y_pos = (-(positives.price - positives.list) / positives.list).median()\n",
    "        num_sold = len(positives.index)\n",
    "        if num_sold == 0:\n",
    "            slack(\"Num of homes with good_sell_prob > %f: %i\\tPerc of those sold: NONE\\tMed disc to list: NA\" % (pos_trigger, num_pos) )    \n",
    "        else:\n",
    "            slack(\"Num of homes with good_sell_prob > %f: %i\\tPerc of those sold: %f\\tMed disc to list: %f\" \n",
    "                  % (pos_trigger, num_pos, num_sold / num_pos *  100, y_pos * 100))\n",
    "\n",
    "        neg_trigger = prior_target_list.good_sell_prob.quantile(0.25)\n",
    "        num_neg = len(prior_target_list[prior_target_list.good_sell_prob < neg_trigger].index)\n",
    "        negatives = backtest[backtest.good_sell_prob < neg_trigger]\n",
    "        y_neg = (-(negatives.price - negatives.list) / negatives.list).median()\n",
    "        num_sold = len(negatives.index)\n",
    "\n",
    "        if num_sold == 0:\n",
    "            slack(\"Num of homes with good_sell_prob < %f: %i\\tPerc of those sold: NONE\\tMedian disc to list: NA\" % (neg_trigger, num_neg))\n",
    "        else:\n",
    "            slack(\"Num of homes with good_sell_prob < %f: %i\\tPerc of those sold: %f\\tMedian disc to list:%f\" \n",
    "                  % (neg_trigger, num_neg, len(negatives.index) / num_neg * 100, y_neg * 100))\n",
    "\n",
    "        slack(backtest.to_string())\n",
    "        backtest.to_csv(\"median_check.csv\")\n",
    "        # store results in arrays for graphing\n",
    "        r = np.array([[lag_days, len(prior_target_list.index), len(backtest.index), num_sold / num_pos * 100, y_pos * 100, len(negatives.index) / num_neg * 100, y_neg * 100]])\n",
    "        model_results = np.append(model_results,r,axis=0)\n",
    "        \n",
    "    except Exception as err:   \n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag days 1\n",
      "\n",
      "\n",
      "Slacking: NN: Prior target list length: 1206\tNum sold: 0\tAvg good sell prob: 0.000453\tAvg good sell prob of sold: nan\n",
      "Slacking: NN: Num of homes with good_sell_prob > 0.000520: 302\tPerc of those sold: NONE\tMed disc to list: NA\n",
      "Slacking: NN: Num of homes with good_sell_prob < 0.000230: 302\tPerc of those sold: NONE\tMedian disc to list: NA\n",
      "Slacking: Empty DataFrame\n",
      "Columns: [address, zipcode, good_sell_prob, list, url, lat, long, date_listed, transaction_date, price, good_sell]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# read in prior target list for backtesting purposes\n",
    "model_results = np.empty([0,7])\n",
    "for lag_days in range(1,2):\n",
    "    print(\"Lag days %i\\n\\n\" % lag_days)\n",
    "    try:\n",
    "        prior_target_list = pd.read_csv(csv_path + 'neural_network/nn_target_list_' + (today-dt.timedelta(days=lag_days)) \\\n",
    "                                    .strftime('%Y%m%d') + '.csv', index_col = [0,1]) \\\n",
    "                                    .rename(columns = { 'ypred' : 'good_sell_prob'})\n",
    "\n",
    "        backtest = prior_target_list.join(closed[['price']], how=\"inner\").sort_values(by='good_sell_prob',ascending=False)\n",
    "        backtest['good_sell'] = (backtest.price >= (backtest.list * (1-discount)))\n",
    "        y_all = prior_target_list.good_sell_prob.mean()\n",
    "        y_sold = backtest.good_sell_prob.mean()\n",
    "\n",
    "        pos_trigger = prior_target_list.good_sell_prob.quantile(0.75)\n",
    "        slack(\"NN: Prior target list length: %i\\tNum sold: %i\\tAvg good sell prob: %f\\tAvg good sell prob of sold: %f\" \n",
    "              % (len(prior_target_list.index), len(backtest.index),y_all, y_sold))\n",
    "\n",
    "        num_pos = len(prior_target_list[prior_target_list.good_sell_prob > pos_trigger].index)\n",
    "        positives = backtest[backtest.good_sell_prob > pos_trigger]\n",
    "        y_pos = (-(positives.price - positives.list) / positives.list).median()\n",
    "        num_sold = len(positives.index)\n",
    "        if num_sold == 0:\n",
    "            slack(\"NN: Num of homes with good_sell_prob > %f: %i\\tPerc of those sold: NONE\\tMed disc to list: NA\" % (pos_trigger, num_pos) )    \n",
    "        else:\n",
    "            slack(\"NN: Num of homes with good_sell_prob > %f: %i\\tPerc of those sold: %f\\tMed disc to list: %f\" \n",
    "                  % (pos_trigger, num_pos, num_sold / num_pos *  100, y_pos * 100))\n",
    "\n",
    "        neg_trigger = prior_target_list.good_sell_prob.quantile(0.25)\n",
    "        num_neg = len(prior_target_list[prior_target_list.good_sell_prob < neg_trigger].index)\n",
    "        negatives = backtest[backtest.good_sell_prob < neg_trigger]\n",
    "        y_neg = (-(negatives.price - negatives.list) / negatives.list).median()\n",
    "        num_sold = len(negatives.index)\n",
    "\n",
    "        if num_sold == 0:\n",
    "            slack(\"NN: Num of homes with good_sell_prob < %f: %i\\tPerc of those sold: NONE\\tMedian disc to list: NA\" % (neg_trigger, num_neg))\n",
    "        else:\n",
    "            slack(\"NN: Num of homes with good_sell_prob < %f: %i\\tPerc of those sold: %f\\tMedian disc to list:%f\" \n",
    "                  % (neg_trigger, num_neg, len(negatives.index) / num_neg * 100, y_neg * 100))\n",
    "\n",
    "        slack(backtest.to_string())\n",
    "        backtest.to_csv(\"median_check.csv\")\n",
    "        # store results in arrays for graphing\n",
    "        r = np.array([[lag_days, len(prior_target_list.index), len(backtest.index), num_sold / num_pos * 100, y_pos * 100, len(negatives.index) / num_neg * 100, y_neg * 100]])\n",
    "        model_results = np.append(model_results,r,axis=0)\n",
    "        \n",
    "    except Exception as err:   \n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_for_sale</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>perc_sold_good</th>\n",
       "      <th>discount_good</th>\n",
       "      <th>perc_sold_bad</th>\n",
       "      <th>discount_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_for_sale  num_sold  perc_sold_good  discount_good  perc_sold_bad  \\\n",
       "1.0  1206.0        0.0       0.0            NaN             0.0             \n",
       "\n",
       "     discount_bad  \n",
       "1.0 NaN            "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = pd.Index(model_results[:,0], names='lag_days')\n",
    "graph_results = pd.DataFrame(model_results[:,1:],index=index,columns=['num_for_sale', 'num_sold', 'perc_sold_good','discount_good', 'perc_sold_bad', 'discount_bad']) \\\n",
    "                    .sort_index(ascending=False)\n",
    "\n",
    "graph_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph_results[['num_for_sale', 'discount_good','discount_bad']].plot(figsize=(14,14),secondary_y='num_for_sale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slacking: Prior target list length: 2183\tNum sold: 0\tAvg P&L: 0.000453\tAvg predict price: nan\tAvg sale price: nan\n",
      "Slacking: Num of homes with pnl > -129037.328125: 546\tPerc of those sold: NONE\tAvg disc to list: NA\n",
      "Slacking: Num of homes with pnl < -189147.914062: 546\tPerc of those sold: NONE\tAvg disc to list: NA\n",
      "Slacking: Empty DataFrame\n",
      "Columns: [address, zipcode, pnl, predicted_price, list, url, price, good_sell]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# read in prior target list for backtesting purposes\n",
    "prior_target_list = pd.read_csv(csv_path + 'value_buy/vb_target_list_' + (today-dt.timedelta(days=lag_days)) \\\n",
    "                                .strftime('%Y%m%d') + '.csv', index_col = [0,1]) \\\n",
    "                                .rename(columns = { 'ypred' : 'pnl'})\n",
    "\n",
    "try:\n",
    "    backtest = prior_target_list.join(closed['price'], how=\"inner\").sort_values(by='pnl',ascending=False)\n",
    "    backtest['good_sell'] = (backtest.price >= (backtest.list * (1-discount)))\n",
    "    avg_pnl = backtest.pnl.mean()\n",
    "    predicted_price = backtest.predicted_price.mean()\n",
    "    sale_price = backtest.price.mean()\n",
    "\n",
    "    slack(\"Prior target list length: %i\\tNum sold: %i\\tAvg P&L: %f\\tAvg predict price: %f\\tAvg sale price: %f\" \n",
    "          % (len(prior_target_list.index), len(backtest.index),y_all, predicted_price, sale_price))\n",
    "\n",
    "    pos_trigger = prior_target_list.pnl.quantile(0.75)\n",
    "    num_pos = len(prior_target_list[prior_target_list.pnl > pos_trigger].index)\n",
    "    positives = backtest[backtest.pnl > pos_trigger]\n",
    "    y_pos = (-(positives.price - positives.list) / positives.list).mean()\n",
    "    num_sold = len(positives.index)\n",
    "    if num_sold == 0:\n",
    "        slack(\"Num of homes with pnl > %f: %i\\tPerc of those sold: NONE\\tAvg disc to list: NA\" % (pos_trigger, num_pos) )    \n",
    "    else:\n",
    "        slack(\"Num of homes with pnl > %f: %i\\tPerc of those sold: %f\\tAvg disc to list: %f\" \n",
    "              % (pos_trigger, num_pos, num_sold / num_pos, y_pos))\n",
    "\n",
    "    neg_trigger = prior_target_list.pnl.quantile(0.25)\n",
    "\n",
    "    num_neg = len(prior_target_list[prior_target_list.pnl < neg_trigger].index)\n",
    "    negatives = backtest[backtest.pnl < neg_trigger]\n",
    "    y_neg = (-(negatives.price - negatives.list) / negatives.list).mean()\n",
    "    num_sold = len(negatives.index)\n",
    "\n",
    "    if num_sold == 0:\n",
    "        slack(\"Num of homes with pnl < %f: %i\\tPerc of those sold: NONE\\tAvg disc to list: NA\" % (neg_trigger, num_neg))\n",
    "    else:\n",
    "        slack(\"Num of homes with pnl < %f: %i\\tPerc of those sold: %f\\tAvg disc to list:%f\" \n",
    "              % (neg_trigger, num_neg, len(negatives.index) / num_neg, y_neg))\n",
    "\n",
    "    slack(backtest.to_string())\n",
    "except Exception as err:   \n",
    "    slack(\"No homes sold since last run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'neural_network/nn_target_list_20171210.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9a592ebd95b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neural_network/nn_target_list_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'property_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transaction_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'zipcode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'ypred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'nn_good_sell_prob'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'good_sell/gs_target_list_'\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'property_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transaction_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'ypred'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'gs_good_sell_prob'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilya/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'neural_network/nn_target_list_20171210.csv' does not exist"
     ]
    }
   ],
   "source": [
    "nn = pd.read_csv('neural_network/nn_target_list_' + today.strftime('%Y%m%d') + '.csv', index_col = ['property_id','transaction_id']) \\\n",
    "       .drop(['address','zipcode', 'url','list'],axis=1) \\\n",
    "       .rename(columns = { 'ypred': 'nn_good_sell_prob'}) \\\n",
    "       .drop_duplicates()\n",
    "    \n",
    "gs = pd.read_csv('good_sell/gs_target_list_'  + today.strftime('%Y%m%d') + '.csv', index_col = ['property_id','transaction_id'])  \\\n",
    "       .rename(columns = { 'ypred' : 'gs_good_sell_prob'}) \\\n",
    "       .drop_duplicates()\n",
    "        \n",
    "nn = nn[~nn.index.duplicated(keep='first')]\n",
    "gs = gs[~gs.index.duplicated(keep='first')]\n",
    "    \n",
    "print(\"Len of neural net\\t%i\" % len(nn.index))\n",
    "print(\"Len of good sell\\t%i\" % len(gs.index))\n",
    "\n",
    "c = pd.concat([nn,gs],axis=1, join='inner')\n",
    "\n",
    "c = c[(c.list < 250000) & (c.list > 100000)  \\\n",
    "      & (~c.address.str.contains(\"Rent|rent|LOT|#|Sun City\"))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = plot_rounds(c.query('gs_good_sell_prob > 0.00').plot.scatter(x='nn_good_sell_prob', y='gs_good_sell_prob'))\n",
    "slack(\"\", url, \"Model Correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for_sale = pd.read_csv('CSV_backups/ALL-for_sale.csv', index_col=['property_id','transaction_id'])\n",
    "for_sale['zipcode'] = for_sale.zipcode.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(row, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lon1 = row.longitude\n",
    "    lat1 = row.latitude\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km / 1.60934 # convert to miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r, target in d.iterrows():\n",
    "    print(target.name[0])\n",
    "    print(target.address)\n",
    "    target.longitude = for_sale.loc[target.name].longitude\n",
    "    target.latitude = for_sale.loc[target.name].latitude\n",
    "    \n",
    "    # get all properties listed for sale in the same zipcode as our target\n",
    "    z = for_sale[(for_sale.zipcode == target.zipcode)]\n",
    "    print(len(z.longitude))\n",
    "    \n",
    "    z['dist'] = z.apply(haversine, lon2 = target.longitude, lat2 = target.latitude, axis=1)\n",
    "    print(z[['address','dist']].nsmallest(5,'dist'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
