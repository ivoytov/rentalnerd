{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Authorize server-to-server interactions from Google Compute Engine.\n",
    "from oauth2client.contrib import gce\n",
    "from httplib2 import Http\n",
    "from apiclient import errors\n",
    "from apiclient.http import MediaFileUpload\n",
    "from apiclient import discovery\n",
    "\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scopes = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    '/home/ilya/Code/secret/python_client_privkey.json', scopes)\n",
    "\n",
    "http_auth = credentials.authorize(Http())\n",
    "service = discovery.build('drive', 'v3', http=http_auth)\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = dt.date.today()\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "path = '/home/ilya/rentalnerd-models/'\n",
    "\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'reg:linear',\n",
    "         'booster': 'gbtree',\n",
    "         'eval_metric':'mae', \n",
    "         'updater': 'grow_gpu',\n",
    "         'eta': 0.1, # not tuned, learning rate with default of 0.3\n",
    "         'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "         'alpha': 2.6456,\n",
    "         'gamma': 6.4589, \n",
    "         'subsample': 0.9893,\n",
    "         'colsample_bytree': 0.6759,\n",
    "         'min_child_weight': 16,\n",
    "         'max_delta_step': 0\n",
    "        }\n",
    "\n",
    "num_round = 50000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def capture():\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    olderr, oldout = sys.stderr, sys.stdout\n",
    "    try:\n",
    "        out = [StringIO(), StringIO()]\n",
    "        sys.stderr, sys.stdout = out\n",
    "        yield out\n",
    "    finally:\n",
    "        sys.stderr, sys.stdout = olderr, oldout\n",
    "        out[0] = out[0].getvalue().splitlines()\n",
    "        out[1] = out[1].getvalue().splitlines()\n",
    "\n",
    "def parse_rounds(result):\n",
    "    import re\n",
    "    pattern = re.compile(r'^\\[(?P<round>\\d+)\\]\\t*(?P<a>\\D+):(?P<tmae>\\-?\\d+.\\d+)\\t*(?P<b>\\D+):(?P<vmae>\\-?\\d+.\\d+)')\n",
    "    xgb_list = []\n",
    "    once = True\n",
    "    for line in (result):\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            idx = int(match.group(\"round\"))\n",
    "            tmae = float(match.group(\"tmae\"))\n",
    "            vmae = float(match.group(\"vmae\"))\n",
    "            xgb_list.append([idx, tmae, vmae])\n",
    "            # grab the column names: we'd like to do this only once\n",
    "            if once:\n",
    "                a = str(match.group(\"a\"))\n",
    "                b = str(match.group(\"b\"))\n",
    "                once = False\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    learning_curve = pd.DataFrame(xgb_list)\n",
    "    learning_curve.columns = ['round',a,b]\n",
    "    learning_curve.set_index('round',inplace=True)\n",
    "    return learning_curve\n",
    "\n",
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n",
    "\n",
    "        \n",
    "def output_model_metrics( x, ypred, y_known, t ):\n",
    "    #Print model report:\n",
    "    mae = metrics.mean_absolute_error(y_known, ypred)\n",
    "    r2 = metrics.explained_variance_score(y_known, ypred)\n",
    "  \n",
    "    slack(\"%s: Model Report:\\t%s \\t n:\\t%i \\t\\t MAE Score:\\t%f \\t\\t R^2:\\t%f\" % (city, t, len(y_known), mae, r2))\n",
    "\n",
    "    \n",
    "def train_model(train, test, factors, xgb_model = None):\n",
    "    dtrain = xgb.DMatrix(train[factors].values, label=train.price, feature_names=factors)\n",
    "    dtest = xgb.DMatrix(test[factors].values, label=test.price, feature_names=factors)\n",
    "    watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "  \n",
    "    with capture() as result:\n",
    "        bst = xgb.train( param, dtrain, num_round, evals = watchlist, xgb_model = xgb_model\n",
    "                        , early_stopping_rounds = 50, verbose_eval = 1 )\n",
    "    \n",
    "    if hasattr(bst, 'best_score'):\n",
    "        print(\"Early stopping occured, best_score %f, best_iteration %i\" % (bst.best_score, bst.best_iteration))\n",
    "\n",
    "    log = parse_rounds(result[1])\n",
    "    url = plot_rounds(log[:-1].plot(logy=True))\n",
    "    slack(\"\", url, \"%s MAE by Round ($)\" % city)\n",
    "    \n",
    "    url = plot_rounds(xgb.plot_importance(bst,max_num_features=20))\n",
    "    slack(\"\", url, \"%s: Feature Importance (n trees)\" % city)\n",
    "        \n",
    "    # predict the training set using the model - note this is in sample testing\n",
    "    ypred = bst.predict(dtrain, ntree_limit=bst.best_iteration)\n",
    "    output_model_metrics( dtrain, ypred, train.price, 'train' )\n",
    "\n",
    "    # predict the testing set using the model\n",
    "    ypred = bst.predict(dtest, ntree_limit=bst.best_iteration)\n",
    "    output_model_metrics( dtest, ypred, test.price, 'test' )\n",
    "    \n",
    "    return bst\n",
    "        \n",
    "def generate_city_model(data):\n",
    "    slack(\"Training sales model for city %s\" % city)\n",
    "    # train model based on historical sales information\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for g, df in data['sales_train'].groupby(np.arange(len(data['sales_train'])) // 150000):  # split the dataset into 100k chunks\n",
    "        bst = train_model(df, data['sales_test'], factors, xgb_model = (bst if g > 0 else None))\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "    \n",
    "    target = xgb.DMatrix( data['for_sale'][factors].values, feature_names=factors)\n",
    "    ypred = bst.predict(target, ntree_limit=bst.best_iteration)\n",
    "    \n",
    "    # save model and delete it to free GPU memory\n",
    "    bst.save_model(path +  city.lower() + '_sales_' + today.strftime('%Y%m%d') + '.model')\n",
    "    \n",
    "    del bst\n",
    "\n",
    "    values = np.column_stack((data['for_sale'].property_id.values\n",
    "                             ,data['for_sale'].address.values\n",
    "                             ,ypred\n",
    "                             ,data['for_sale'].price.values\n",
    "                             ,ypred-data['for_sale'].price\n",
    "                             ,ypred / data['for_sale'].price - 1\n",
    "                             ,data['for_sale']['origin_url'].values))\n",
    "    output = pd.DataFrame(values[:,1:],index=values[:,0],columns=['address','ypred','list','gain-loss', 'gain-loss%','url'])\n",
    "    output = output.sort_values(by='gain-loss',ascending=False)\n",
    "\n",
    "    # train rental model\n",
    "    start_time = timeit.default_timer() # start timer\n",
    "    rent_bst = train_model(data['rent_train'], data['rent_test'], factors)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time # end timer\n",
    "    slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "\n",
    "    # predict rent prices for home that are listed for sale\n",
    "    ypred = rent_bst.predict(target, ntree_limit=rent_bst.best_iteration)\n",
    "    ypred = pd.Series(ypred,index=output.index)\n",
    "    ypred.name = \"rent\"\n",
    "    \n",
    "    # save rental model and delete it from GPU memory\n",
    "    rent_bst.save_model(path + city.lower() + '_rent_' + today.strftime('%Y%m%d') + '.model')\n",
    "    del rent_bst\n",
    "\n",
    "    # calculate estimated cap rate\n",
    "    cr = ypred * 12 / output.list\n",
    "    cr.name = \"cap rate\"\n",
    "\n",
    "    # combine rent predictions to homes listed for sale\n",
    "    best_of = pd.concat([output,ypred, cr],axis=1)\n",
    "\n",
    "    # save target list\n",
    "    file = city + '_target_list.csv'\n",
    "    profit_goal = -10000\n",
    "    best_of = best_of[best_of[\"gain-loss\"] > profit_goal]\n",
    "    best_of.to_csv(file)\n",
    "    slacker.files.upload(file, channels='#progress')\n",
    "    slack(\"Number of targets with estimated profit over $%i: %i\" % (profit_goal, len(best_of.index)))\n",
    "\n",
    "def city_query():\n",
    "    query = %sql (\\\n",
    "    SELECT area_name, COUNT(id) \\\n",
    "    FROM area_name_zipcodes \\\n",
    "    GROUP BY area_name \\\n",
    "    ORDER BY 2 DESC \\\n",
    "    limit 100)\n",
    "    return query.DataFrame().area_name.values\n",
    "    \n",
    "def queue_city_reads(city):\n",
    "    # read in all of the files in the same order we ran queries\n",
    "    sales = pd.read_csv('CSV_backups/' + city + '-sales.csv')\n",
    "    for_sale = pd.read_csv('CSV_backups/' + city + '-for_sale.csv')\n",
    "    rentals = pd.read_csv('CSV_backups/' + city + '-rentals.csv')\n",
    "    \n",
    "    sales_train, sales_test = cv.train_test_split(sales, test_size = 0.2)\n",
    "    del sales\n",
    "    rent_train, rent_test = cv.train_test_split(rentals, test_size = 0.2)\n",
    "    del rentals\n",
    "\n",
    "    data = {'sales_train': sales_train\n",
    "            , 'sales_test': sales_test\n",
    "            , 'rent_train': rent_train\n",
    "            , 'rent_test': rent_test\n",
    "            , 'for_sale': for_sale }\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of city models to run: ['ALL']\n"
     ]
    }
   ],
   "source": [
    "# get list of all available cities\n",
    "cities = ['ALL'] #city_query()\n",
    "print(\"Order of city models to run:\", cities)\n",
    "cut_off_date = ((today - dt.timedelta(6*365/12)) - dt.date(2000, 1, 1)).days\n",
    "\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_data[city] = queue_city_reads(city)\n",
    "    \n",
    "cols = city_data['ALL']['sales_train'].columns\n",
    "ind2remove = ['Unnamed: 0', 'address', 'area_name', 'date_listed', 'id', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'zestimate_sale']\n",
    "factors = np.setdiff1d(cols, ind2remove)\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    print(\"Generating model for city\", city)\n",
    "    generate_city_model(city_data[city])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
