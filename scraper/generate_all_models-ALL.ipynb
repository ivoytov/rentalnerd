{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import timeit  # for timing models\n",
    "import contextlib\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "# Authorize server-to-server interactions from Google Compute Engine.\n",
    "from oauth2client.contrib import gce\n",
    "from httplib2 import Http\n",
    "from apiclient import errors\n",
    "from apiclient.http import MediaFileUpload\n",
    "from apiclient import discovery\n",
    "\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# today's date for output filenames\n",
    "today = dt.date.today()\n",
    "\n",
    "# where to save the xgb models - they get huge so keep them out of any git path\n",
    "path = '/home/ilya/rentalnerd-models/'\n",
    "\n",
    "\n",
    "# booster parameters\n",
    "param = {'verbose': 0,\n",
    "         'silent': 0,\n",
    "         'objective':'reg:linear',\n",
    "         'booster': 'gbtree',\n",
    "         'eval_metric':'error', \n",
    "         'updater': 'grow_gpu',\n",
    "         'eta': 0.1, # not tuned, learning rate with default of 0.3\n",
    "         'max_depth': 10,  # all of the following parameters are __tuned__ so do not change them\n",
    "         'alpha': 2.6456,\n",
    "         'gamma': 6.4589, \n",
    "         'subsample': 0.9893,\n",
    "         'colsample_bytree': 0.6759,\n",
    "         'min_child_weight': 16,\n",
    "         'max_delta_step': 0\n",
    "        }\n",
    "\n",
    "num_round = 5000 # pick a high number - XGB will abort as soon as accuracy drops in the testing set\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def capture():\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    olderr, oldout = sys.stderr, sys.stdout\n",
    "    try:\n",
    "        out = [StringIO(), StringIO()]\n",
    "        sys.stderr, sys.stdout = out\n",
    "        yield out\n",
    "    finally:\n",
    "        sys.stderr, sys.stdout = olderr, oldout\n",
    "        out[0] = out[0].getvalue().splitlines()\n",
    "        out[1] = out[1].getvalue().splitlines()\n",
    "\n",
    "def parse_rounds(result):\n",
    "    import re\n",
    "    pattern = re.compile(r'^\\[(?P<round>\\d+)\\]\\t*(?P<a>\\D+):(?P<tmae>\\-?\\d+.\\d+)\\t*(?P<b>\\D+):(?P<vmae>\\-?\\d+.\\d+)')\n",
    "    xgb_list = []\n",
    "    once = True\n",
    "    for line in (result):\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            idx = int(match.group(\"round\"))\n",
    "            tmae = float(match.group(\"tmae\"))\n",
    "            vmae = float(match.group(\"vmae\"))\n",
    "            xgb_list.append([idx, tmae, vmae])\n",
    "            # grab the column names: we'd like to do this only once\n",
    "            if once:\n",
    "                a = str(match.group(\"a\"))\n",
    "                b = str(match.group(\"b\"))\n",
    "                once = False\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    learning_curve = pd.DataFrame(xgb_list)\n",
    "    learning_curve.columns = ['round',a,b]\n",
    "    learning_curve.set_index('round',inplace=True)\n",
    "    return learning_curve\n",
    "\n",
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n",
    "\n",
    "        \n",
    "def output_model_metrics( x, ypred, y_known, t ):\n",
    "    #Print model report:\n",
    "    mae = metrics.mean_absolute_error(y_known, ypred)\n",
    "    r2 = metrics.explained_variance_score(y_known, ypred)\n",
    "  \n",
    "    slack(\"%s: Model Report:\\t%s \\t n:\\t%i \\t\\t MAE Score:\\t%f \\t\\t R^2:\\t%f\" % (city, t, len(y_known), mae, r2))\n",
    "\n",
    "    \n",
    "def train_model(train, test, factors, label, xgb_model = None):\n",
    "    dtrain = xgb.DMatrix(train[factors].values, label=train[label], feature_names=factors)\n",
    "    dtest = xgb.DMatrix(test[factors].values, label=test[label], feature_names=factors)\n",
    "    watchlist  = [(dtrain,'train'),(dtest,'eval')]\n",
    "  \n",
    "    with capture() as result:\n",
    "        xgb_model = xgb.train( param, dtrain, num_round, evals = watchlist, xgb_model = xgb_model\n",
    "                        , early_stopping_rounds = 10, verbose_eval = 1 )\n",
    "        \n",
    "    if hasattr(xgb_model, 'best_score'):\n",
    "        slack(\"Early stopping occured, best_score %f, best_iteration %i\" % (xgb_model.best_score, xgb_model.best_iteration))\n",
    "\n",
    "    log = parse_rounds(result[1])\n",
    "    url = plot_rounds(log[:-1].plot(logy=True))\n",
    "    slack(\"\", url, \"%s MAE by Round ($)\" % city)\n",
    "    \n",
    "    url = plot_rounds(xgb.plot_importance(xgb_model,max_num_features=20))\n",
    "    slack(\"\", url, \"%s: Feature Importance (n trees)\" % city)\n",
    "        \n",
    "    # predict the training set using the model - note this is in sample testing\n",
    "    ypred = xgb_model.predict(dtrain, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtrain, ypred, train[label], 'train' )\n",
    "\n",
    "    # predict the testing set using the model\n",
    "    ypred = xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    output_model_metrics( dtest, ypred, test[label], 'test' )\n",
    "    \n",
    "    # clean out the model from memory\n",
    "    xgb_model.save_model(path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "    del xgb_model\n",
    "    gc.collect()\n",
    "        \n",
    "def generate_city_model(sales_train, sales_test, for_sale):\n",
    "    # cap number of homes that fit into VRAM\n",
    "    memory_cap = 250000\n",
    "    \n",
    "    # init empty model that we can load into on the second iteration\n",
    "    bst = xgb.Booster()\n",
    "    \n",
    "    # first run the price model\n",
    "    label = 'price'\n",
    "    f = factors\n",
    "#     f.remove(label) # this happens in place\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for g, df in sales_train.groupby(np.arange(limit) // memory_cap):  # split the dataset into 250k chunks    \n",
    "        train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "        \n",
    "        # load the model into memory - should have been saved by train_model function\n",
    "        bst.load_model(path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "    \n",
    "    target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "    ypred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else None))\n",
    "\n",
    "    # second run the days on the market model\n",
    "    sales_train = sales_train[(sales.days_on_market > 0 )]\n",
    "    sales_test = sales_test[(sales.days_on_market > 0 )]\n",
    "    label = 'days_on_market'\n",
    "    f = factors\n",
    "    f.remove(label)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    for g, df in sales_train.groupby(np.arange(limit) // memory_cap):  # split the dataset into 100k chunks    \n",
    "        train_model(df, sales_test, f, label, xgb_model = (bst if g > 0 else None))\n",
    "        \n",
    "        # load the model into memory - should have been saved by train_model function\n",
    "        bst.load_model(path +  'all_' + label + '_' + today.strftime('%Y%m%d') + '.model')\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    slack(\"%s:\\tTime to train:\\t%f minutes\" % (city, (elapsed / 60)))\n",
    "    \n",
    "    target = xgb.DMatrix( for_sale[f].values, feature_names=f)\n",
    "    dayspred = bst.predict(target, ntree_limit=(bst.best_iteration if hasattr(bst, 'best_score') else None))\n",
    "\n",
    "    \n",
    "    values = np.column_stack((for_sale.property_id.values\n",
    "                             ,for_sale.address.values\n",
    "                             ,ypred\n",
    "                             ,for_sale.price.values\n",
    "                             ,ypred-for_sale.price\n",
    "                             ,ypred / for_sale.price - 1\n",
    "                             ,dayspred\n",
    "                             ,for_sale['origin_url'].values))\n",
    "    output = pd.DataFrame(values[:,1:],index=values[:,0],columns=['address','ypred',\n",
    "                                                                  'list','gain-loss', 'gain-loss%','days_pred', 'url'])\n",
    "    output = output.sort_values(by='gain-loss',ascending=False)\n",
    "\n",
    "    # save target list\n",
    "    file = 'all_target_list.csv'\n",
    "    output.to_csv(file)\n",
    "    slacker.files.upload(file, channels='#progress')\n",
    "\n",
    "def queue_reads():\n",
    "    # read in all of the files in the same order we ran queries\n",
    "    sales = pd.read_csv('CSV_backups/' + city + '-sales.csv',nrows=limit)\n",
    "    for_sale = pd.read_csv('CSV_backups/' + city + '-for_sale.csv',nrows=limit)\n",
    "    \n",
    "    sales_train, sales_test = cv.train_test_split(sales, test_size = 0.2) # set aside X% of the dataset for testing\n",
    "    del sales\n",
    "\n",
    "    return sales_train, sales_test, for_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "cut_off_date = ((today - dt.timedelta(6*365/12)) - dt.date(2000, 1, 1)).days\n",
    "\n",
    "city = 'ALL'\n",
    "limit = 10000000\n",
    "\n",
    "sales_train, sales_test, for_sale = queue_reads()\n",
    "    \n",
    "limit = min(limit, len(sales_train.index))\n",
    "    \n",
    "ind2remove = ['Unnamed: 0', 'address', 'area_name', 'date_listed', 'id', 'listed_diff_id', 'lookup_address',\n",
    "              'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "              'images']\n",
    "factors = np.setdiff1d(sales_train.columns, ind2remove).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slacking: Early stopping occured, best_score -185402.937500, best_iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/lib/python3.6/site-packages/matplotlib/ticker.py:2041: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  \"Data has no positive values, and therefore cannot be \"\n",
      "/home/ilya/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slacking: \n"
     ]
    }
   ],
   "source": [
    "generate_city_model(sales_train, sales_test, for_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
