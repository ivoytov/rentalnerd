{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on: https://github.com/fmfn/BayesianOptimization/blob/master/examples/xgboost_example.py\n",
    "Computes the best parameters for XGB model optimization\n",
    "'''\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from slacker import Slacker\n",
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "import os\n",
    "# slack secrets (in your ~/.bashrc)\n",
    "webhook_url = os.environ.get('SLACK_URL')\n",
    "slacker = Slacker(os.environ.get('SLACK_TOKEN'))\n",
    "\n",
    "%load_ext sql\n",
    "# %sql mysql://root@localhost/rental_nerd\n",
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd\n",
    "limit = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def XGBcv(max_depth, gamma, min_child_weight, max_delta_step, subsample, colsample_bytree, alpha):\n",
    "    folds = 5\n",
    "    paramt = {\n",
    "        'eta': 0.05,\n",
    "        'verbose_eval': 1,\n",
    "        'silent': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': 'gbtree',\n",
    "        'eval_metric': 'error',\n",
    "        'updater': 'grow_gpu',\n",
    "#         'eta': max(eta, 0),\n",
    "        'max_depth': int(max_depth),\n",
    "        'alpha': max(alpha, 0),\n",
    "        'gamma': max(gamma, 0),\n",
    "        'subsample': max(min(subsample, 1), 0),\n",
    "        'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "        'min_child_weight': int(min_child_weight),\n",
    "        'max_delta_step': int(max_delta_step)\n",
    "    }\n",
    "    \n",
    "    out = xgb.cv(paramt,\n",
    "           dtrain,\n",
    "           num_boost_round=3000,\n",
    "           folds=tscv.split(dtrain),\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    \n",
    "    output = out['test-error-mean'].values[-1]\n",
    "    del out\n",
    "    gc.collect()\n",
    "    \n",
    "    return output\n",
    "\n",
    "def sanitize(data, zipcode_list = None):\n",
    "    # abort if the city has no top zipcodes\n",
    "    if data.empty:\n",
    "        return 0    \n",
    "    \n",
    "    data.drop(['abnormal', 'bookmarked', 'created_at', 'ignore', 'is_latest', 'id', 'closed_diff_id', 'listed_diff_id',\n",
    "                     'notes', 'source', 'updated_at', 'home_type', 'sfh', 'description', \n",
    "                   'event_name', 'neighborhood'], axis=1, inplace=True)\n",
    "    \n",
    "    # filters out any non-sensical values or fat finger mistakes in MLS listings\n",
    "    print(\"Entries before filter: \", len(data))\n",
    "\n",
    "    if(data.transaction_type.iloc[0] == 'sales'):\n",
    "        data = data[ data.price > 50000 ]\n",
    "    else:\n",
    "        data = data [ data.price > 500 ]\n",
    "    \n",
    "    if(zipcode_list is not None):\n",
    "        data = data[data.zipcode.isin(zipcode_list)]\n",
    "        \n",
    "#     data = data [ (data.price_closed - data.price_listed).abs() < 50000 ]\n",
    "\n",
    "    slack(\"Entries after filter: %i\" % len(data))\n",
    "    \n",
    "    # fills in some sensible defaults where data is missing\n",
    "    data[\"near_golf_course\"] = data[\"near_golf_course\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data[\"has_pool\"] = data[\"has_pool\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data[\"garage\"] = data[\"garage\"].apply(lambda x: True if x == 1.0 else False)\n",
    "    data['date_closed'] = data['date_closed'].apply(lambda x: 0 if x == None else (x - dt.date(2000, 1, 1)).days)\n",
    "    data['date_closed'] = data['date_closed'].astype(int)\n",
    "    \n",
    "    # convert the area name into dummy variables\n",
    "    dm = pd.get_dummies(data[['area_name', 'zipcode']], prefix=['area_name','zipcode'])\n",
    "    data = pd.concat([data, dm], axis=1)\n",
    "    del dm\n",
    "    \n",
    "    return data\n",
    "\n",
    "def slack(text, url = None, title = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": title\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = %sql (\\\n",
    "    SELECT \\\n",
    "    area_name_zipcodes.area_name, \\\n",
    "    properties.*, \\\n",
    "    property_transaction_logs.id as 'transaction_id', \\\n",
    "    property_transaction_logs.* \\\n",
    "    FROM  \\\n",
    "    property_transaction_logs \\\n",
    "    LEFT JOIN \\\n",
    "    properties on properties.id = property_transaction_logs.`property_id`  \\\n",
    "    LEFT JOIN \\\n",
    "    area_name_zipcodes on properties.zipcode = area_name_zipcodes.zipcode \\\n",
    "    where \\\n",
    "    home_type = 'sfh' AND \\\n",
    "    properties.sqft between 1 and 10000 and \\\n",
    "    ( abnormal = false OR abnormal IS NULL OR abnormal = 0 ) and \\\n",
    "    property_transaction_logs.price between 500 and 400000 and \\\n",
    "    properties.bedrooms <= 6 and \\\n",
    "    properties.bathrooms <= 6 and \\\n",
    "    transaction_type = 'sales' and  \\\n",
    "    date_closed is not null and \\\n",
    "    price_closed is not null and \\\n",
    "    days_on_market is not null and \\\n",
    "    transaction_status = 'closed' \\\n",
    "    ORDER BY property_transaction_logs.date_closed DESC \\\n",
    "    LIMIT :limit )\n",
    "\n",
    "\n",
    "df = query.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('property_id', inplace=True)\n",
    "df.index.name = 'property_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_values = {'target': [ 0.05,  0.05], 'alpha': [ 8.97,  9.99], 'colsample_bytree': [ 0.35,  0.26], 'gamma': [ 9.37,  6.42], 'max_delta_step': [ 0.09,  2.86], 'max_depth': [ 14.6,  10. ], 'min_child_weight': [ 19.96,   6.34], 'subsample': [ 0.8,  0.8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df = sanitize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {       'max_depth': (5, 25),\n",
    "                 'gamma': (0.0, 20.0),\n",
    "                 'min_child_weight': (1, 40),\n",
    "                 'max_delta_step': (0, 10),\n",
    "                 'subsample': (0.1, 5.0),\n",
    "                 'colsample_bytree' :(0.01, 1.0),\n",
    "                 'alpha': (0, 20)\n",
    "               }\n",
    "\n",
    "XGB_BOpt = BayesianOptimization(XGBcv, params)\n",
    "XGB_BOpt.initialize(init_values)\n",
    "\n",
    "discount = .10\n",
    "df['good_sell'] = (df.price_closed >= (df.price_listed * (1 - discount )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "# ind2remove = ['Unnamed: 0', 'address', 'area_name', 'date_listed', 'id', 'listed_diff_id', 'lookup_address',\n",
    "#               'origin_url', 'neighborhood', 'zipcode', 'luxurious', 'transaction_status', 'transaction_type',\n",
    "#               'zestimate_sale']\n",
    "ind2remove = ['area_name', 'address', 'origin_url', 'lookup_address', 'zipcode', 'zestimate_rent', 'date_listed', 'date_closed', \n",
    "              'zestimate_sale', 'construction', 'stories', 'transaction_id', 'transaction_status', 'good_sell', \n",
    "              'transaction_type', 'price_closed', 'date_transacted_latest', 'updated_at','notes','price']\n",
    "\n",
    "factors = np.setdiff1d(cols, ind2remove)\n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(df[factors].values, label=df.good_sell, feature_names=factors)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# per link below i need to use Upper Confidence Bound and add some alpha (square of stdev), otherwise it starts to loop\n",
    "# https://github.com/fmfn/BayesianOptimization/issues/10 \n",
    "XGB_BOpt.maximize(init_points=5, n_iter=100, acq='ucb', kappa=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not used - reset the variable\n",
    "#new_init = { 'target': [], 'alpha': [], 'colsample_bytree': [], 'gamma': [], 'max_delta_step': [], 'max_depth': [], 'min_child_weight': [], 'subsample': [] }\n",
    "new_init = init_values\n",
    "\n",
    "# store resulting values to help seed the next run. make sure not to overwrite but add incrementally\n",
    "# copy paste the print out of init_values into the cell above\n",
    "for i in range(len(XGB_BOpt.res['all']['values'])):\n",
    "    new_init['target'].append(XGB_BOpt.res['all']['values'][i])\n",
    "    for k,v in XGB_BOpt.res['all']['params'][i].items():\n",
    "        new_init[k].append(np.round(v,decimals=2).values)\n",
    "    \n",
    "print (new_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(XGB_BOpt.res['max'])\n",
    "(pd.DataFrame(XGB_BOpt.res['all']['values'])*-1.0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from cloudinary.uploader import upload\n",
    "from cloudinary.utils import cloudinary_url\n",
    "from cloudinary.api import delete_resources_by_tag, resources_by_tag\n",
    "\n",
    "def plot_rounds(plot):\n",
    "    # uploads the graph to the web and returns the URL\n",
    "    \n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('temp_plot.png')\n",
    "    \n",
    "    response = upload(\"temp_plot.png\")\n",
    "    url, options = cloudinary_url(response['public_id'],\n",
    "        format = response['format'],\n",
    "        crop = \"fill\")\n",
    "    return url\n",
    "\n",
    "def slack(text, url = None):\n",
    "    print(\"Slacking: \" + text)\n",
    "    \n",
    "    if url == None:\n",
    "        data=json.dumps({\"text\": text})\n",
    "    else:\n",
    "        data = json.dumps( { \"text\": text, \"attachments\": [ { \"fallback\": \"Model MAE\"\n",
    "                                           , \"title\": \"Model Mean Average Error by Iteration ($)\"\n",
    "                                           , \"image_url\": url } ] } )\n",
    "    \n",
    "    response = requests.post(webhook_url, data , headers={'Content-Type': 'application/json'})\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError('Request to slack returned an error %s, the response is:\\n%s' % (response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(XGB_BOpt.res['all']['params'])\n",
    "error = pd.Series(XGB_BOpt.res['all']['values']) * -1\n",
    "error.name = 'test-error'\n",
    "result = pd.concat([error, result], axis=1)\n",
    "result.head(25)\n",
    "\n",
    "url = plot_rounds(error.plot())\n",
    "slack(\"Bayesian Search: Max params %s\" % XGB_BOpt.res['max'], url)\n",
    "\n",
    "file = 'ALL-bayesian-parameters.csv'\n",
    "result.to_csv(file)\n",
    "slacker.files.upload(file, channels='#progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
