{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "import statsmodels.api as sma\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap # Basemap must be imported before Shapely due to conflict\n",
    "import fiona\n",
    "import shapely as shapely\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import asShape\n",
    "from time import gmtime, strftime\n",
    "from array import array\n",
    "\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# follow the usual sklearn pattern: import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# city abbreviation code\n",
    "city = 'las'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql mysql://prod:nerd@52.2.153.189/rental_nerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = %sql (\\\n",
    "SELECT  \\\n",
    "properties.id as 'property_id', \\\n",
    "properties.address,  \\\n",
    "properties.bedrooms,  \\\n",
    "properties.bathrooms,  \\\n",
    "properties.sqft,  \\\n",
    "properties.elevation,  \\\n",
    "(2016 - properties.year_built) as 'age',  \\\n",
    "properties.garage,  \\\n",
    "properties.level,  \\\n",
    "properties.zipcode, \\\n",
    "properties.has_pool, \\\n",
    "properties.home_type, \\\n",
    "properties.neighborhood, \\\n",
    "property_transaction_logs.id 'ptl_id',  \\\n",
    "property_transaction_logs.transaction_type,  \\\n",
    "property_transaction_logs.price,  \\\n",
    "property_transaction_logs.transaction_status,  \\\n",
    "property_transaction_logs.days_on_market,  \\\n",
    "property_transaction_logs.date_closed as 'date',  \\\n",
    "property_transaction_logs.date_listed \\\n",
    "FROM  \\\n",
    "properties,  \\\n",
    "property_transaction_logs \\\n",
    "WHERE  \\\n",
    "property_transaction_logs.property_id = properties.id AND  \\\n",
    "property_transaction_logs.transaction_type = \"rental\" AND  \\\n",
    "property_transaction_logs.date_closed is not null AND \\\n",
    "properties.zipcode LIKE '89%' AND \\\n",
    "properties.sqft > 0 AND \\\n",
    "properties.bedrooms IS NOT NULL AND \\\n",
    "properties.bathrooms IS NOT NULL AND \\\n",
    "properties.elevation IS NOT NULL AND \\\n",
    "properties.level IS NOT NULL AND \\\n",
    "properties.home_type IS NOT NULL AND \\\n",
    "properties.zipcode IS NOT NULL AND \\\n",
    "properties.sqft IS NOT NULL AND \\\n",
    "properties.year_built IS NOT NULL AND \\\n",
    "property_transaction_logs.price > 0 )\n",
    "           \n",
    "    \n",
    "# properties.has_pool IS NOT NULL AND \\\n",
    "# properties.garage IS NOT NULL AND \\\n",
    "data = query.DataFrame()\n",
    "data_copy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"year\"] = pd.DatetimeIndex(data[\"date\"]).to_period('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"has_pool\"] = data[\"has_pool\"].apply(lambda x: True if x == 1.0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out any outliers\n",
    "\n",
    "print \"Entries before filter: \" + `len(data)`\n",
    "data = data[  (data.sqft <= 10000) \n",
    "            & (data.price <= 6000) \n",
    "            & (data.price > 400)\n",
    "            & (data.bedrooms <= 6) \n",
    "            & (data.bathrooms <= 6)]\n",
    "\n",
    "print \"Entries after filter: \" + `len(data)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe() #identify filtering tresholds to clean up bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.hist(column=['has_pool','bathrooms','bedrooms','price','garage','level','age','sqft','elevation'],figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adjust variables so that for most houses they result in zero (or close to zero)\n",
    "print \"minimum elevation: \" + `data.elevation.min()`\n",
    "data.elevation = data.elevation - data.elevation.min()\n",
    "data.level = data.level - 1\n",
    "data.bathrooms = data.bathrooms - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook. \"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Artnet white paper index converted to our dataset\n",
    "\n",
    "# create year dummy variables (because date isn't very intuitive variable)\n",
    "z = 'A'\n",
    "data[\"period\"] = pd.DatetimeIndex(data[\"date\"]).to_period(z)\n",
    "data['period_literal'] = pd.DatetimeIndex(data[\"date\"]).to_period(z).format()\n",
    "\n",
    "paired = data[['address','date','price','period','period_literal','zipcode', 'neighborhood']]\n",
    "\n",
    "# identify the earliest date, number of periods, and number of pairs\n",
    "base_period = paired.period.min()\n",
    "num_periods = paired.period.max() - paired.period.min()\n",
    "print \"base period: \" + `base_period` + \" end period: \" + `paired.period.max()` + \" and number of periods: \" + `num_periods`\n",
    "\n",
    "# group data into Sets and calc Y_ist of each item\n",
    "paired = paired.groupby(\"address\").filter(lambda x: len(x) >1)\n",
    "paired.sort_values(['address','period'],inplace=True)\n",
    "paired_grp = paired.groupby('address')\n",
    "print 'number of paired transactions in the data: ' + `paired.shape[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def demean(group):\n",
    "    return pd.DataFrame({'address': group.address, 'original' : group.price, 'demeaned' : group.price - group.price.mean()})\n",
    "\n",
    "# filter out properties with multiple listings in the same year and listings that moved by 20% or more\n",
    "paired = paired.groupby(['address','period_literal']).filter(lambda x: len(x) == 1)\n",
    "paired_grp = paired.groupby('address')\n",
    "n = paired.groupby(['address']).apply(demean)\n",
    "n = n[((n.demeaned / n.original).abs() > 0.10)]\n",
    "\n",
    "paired = paired[~paired.address.isin(n.address)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boundary = 'neighborhood' # 'zipcode' or 'neighborhood'\n",
    "\n",
    "table = ListTable()\n",
    "table.append([boundary,'Period','Growth Rate','P Value','n'])\n",
    "\n",
    "sorted_zips = []\n",
    "\n",
    "# index used to calculate adjusted prices. \n",
    "iterables = [data[boundary].unique(), data.period.unique()]\n",
    "mi = pd.MultiIndex.from_product(iterables, names=[boundary, 'period'])\n",
    "price_adjustment_index = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = sm.ols(formula=\"np.log(price) ~ period + address\", data=paired).fit()\n",
    "\n",
    "#calculate index\n",
    "linked = res.params[res.params.index.str.contains('Period')]\n",
    "linked.name = \"Index\"\n",
    "linked[0] = 100\n",
    "num = pd.Series(linked, copy=True)\n",
    "num[0] = 0\n",
    "num.name = \"n\"\n",
    "growth = pd.Series(linked, copy=True)\n",
    "growth.name = \"Growth Rate\"\n",
    "growth[0] = 0\n",
    "for i in range(1,len(linked)):\n",
    "    linked[i] = (np.exp(res.params[i]))*100\n",
    "    growth[i] = linked[i]/linked[i-1] - 1\n",
    "    num[i] = len(paired[paired.period_literal == filter(str.isdigit, linked.index[i])])\n",
    "\n",
    "# add P values of each prediction\n",
    "p = res.pvalues[res.params.index.str.contains('Period')] * 100\n",
    "p.name = \"P value\"\n",
    "index = pd.concat([linked, growth, p, num], axis=1)\n",
    "index.index = pd.to_datetime(index.index.str.split(\"'\").str.get(1)).to_period(z)\n",
    "\n",
    "print index\n",
    "# citywide index (will add neighborhoods in later calcs)\n",
    "price_adjustment_index['city'] = index['Index']  \n",
    "\n",
    "\n",
    "\n",
    "index[['Index']].plot()\n",
    "# index[['Growth Rate']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price_adjustment_index['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *  \n",
    "def hurst(p):  \n",
    "    tau = []; lagvec = []  \n",
    "    #  Step through the different lags  \n",
    "    for lag in range(2,20):  \n",
    "        #  produce price difference with lag  \n",
    "        pp = subtract(p[lag:],p[:-lag])  \n",
    "        #  Write the different lags into a vector  \n",
    "        lagvec.append(lag)  \n",
    "        #  Calculate the variance of the differnce vector  \n",
    "        tau.append(sqrt(std(pp)))  \n",
    "    #  linear fit to double-log graph (gives power)  \n",
    "    m = polyfit(log10(lagvec),log10(tau),1)  \n",
    "    # calculate hurst  \n",
    "    hurst = m[0]*2  \n",
    "    # plot lag vs variance  \n",
    "    plt.plot(lagvec,tau,'o')\n",
    "    return hurst  \n",
    "if __name__==\"__main__\":  \n",
    "    #  Different types of time series for testing  \n",
    "#     p = log10(cumsum(random.randn(50000)+1)+1000) # trending, hurst ~ 1  \n",
    "    #p = log10((random.randn(50000))+1000)   # mean reverting, hurst ~ 0  \n",
    "#     p = log10(cumsum(random.randn(50000))+1000) # random walk, hurst ~ 0.5  \n",
    "    print hurst(price_adjustment_index['city']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def normcdf(X):\n",
    "    (a1,a2,a3,a4,a5) = (0.31938153, -0.356563782, 1.781477937, -1.821255978, 1.330274429)\n",
    "    L = abs(X)\n",
    "    K = 1.0 / (1.0 + 0.2316419 * L)\n",
    "    w = 1.0 - 1.0 / sqrt(2*pi)*exp(-L*L/2.) * (a1*K + a2*K*K + a3*pow(K,3) + a4*pow(K,4) + a5*pow(K,5))\n",
    "    if X<0:\n",
    "        w = 1.0-w\n",
    "    return w\n",
    " \n",
    " \n",
    "def vratio(a, lag = 2, cor = 'hom'):\n",
    "    t = (std((a[lag:]) - (a[1:-lag+1])))**2;\n",
    "    b = (std((a[2:]) - (a[1:-1]) ))**2;\n",
    " \n",
    "    n = float(len(a))\n",
    "    mu  = sum(a[1:n]-a[:-1])/n;\n",
    "    m=(n-lag+1)*(1-lag/n);\n",
    "#   print mu, m, lag\n",
    "    b=sum(square(a[1:n]-a[:n-1]-mu))/(n-1)\n",
    "    t=sum(square(a[lag:n]-a[:n-lag]-lag*mu))/m\n",
    "    vratio = t/(lag*b);\n",
    " \n",
    "    la = float(lag)\n",
    "     \n",
    " \n",
    "    if cor == 'hom':\n",
    "        varvrt=2*(2*la-1)*(la-1)/(3*la*n)\n",
    " \n",
    " \n",
    "    elif cor == 'het':\n",
    "          varvrt=0;\n",
    "          sum2=sum(square(a[1:n]-a[:n-1]-mu)); \n",
    "          for j in range(lag-1):\n",
    "             sum1a=square(a[j+1:n]-a[j:n-1]-mu); \n",
    "             sum1b=square(a[1:n-j]-a[0:n-j-1]-mu)\n",
    "             sum1=dot(sum1a,sum1b); \n",
    "             delta=sum1/(sum2**2);\n",
    "             varvrt=varvrt+((2*(la-j)/la)**2)*delta\n",
    " \n",
    "    zscore = (vratio - 1) / sqrt(float(varvrt))\n",
    "    pval = normcdf(zscore);\n",
    " \n",
    "    return  vratio, zscore, pval\n",
    " \n",
    "if __name__==\"__main__\":\n",
    " \n",
    "    trend = 0.25  #  The larger this number the stronger the trend, hence larger zscore and pval\n",
    "    bias = 1000   #  This is to make sure that the time series does not go negative\n",
    "    a = log((random.randn(1000)+bias)); #Mean reverting: hurst very small, vratio should be small\n",
    "#    a = log(cumsum(random.randn(10000)+trend)+bias); #trending: hurst > 0.5;\n",
    "#    a = log(cumsum(random.randn(10000)) + bias); #random walk: hurst ~ 0.5\n",
    " \n",
    "    print vratio(price_adjustment_index['city'].values, cor = 'het', lag = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for hood in paired[boundary].unique():\n",
    "    sorted_zips.append([hood, len(paired[paired[boundary] == hood])])\n",
    "    \n",
    "for hood, n in sorted(sorted_zips, key =lambda x: x[1], reverse=True):\n",
    "    d = paired[paired[boundary] == hood]\n",
    "    if len(d) < 50:\n",
    "        print 'only ' + `len(d)` + 'transactions in ' + hood\n",
    "        continue\n",
    "\n",
    "    res = sm.ols(formula=\"np.log(price) ~ period + address\", data=d).fit()\n",
    "\n",
    "    #calculate index\n",
    "    linked = res.params[res.params.index.str.contains('Period')]\n",
    "    linked.name = \"Index\"\n",
    "    linked[0] = 100\n",
    "    \n",
    "    growth = pd.Series(linked, copy=True)\n",
    "    growth.name = \"Growth Rate\"\n",
    "    growth[0] = 0\n",
    "    for i in range(1,len(linked)):\n",
    "        linked[i] = (np.exp(res.params[i]))*100\n",
    "        growth[i] = linked[i]/linked[i-1] - 1\n",
    "\n",
    "    # add P values of each prediction\n",
    "    p = res.pvalues[res.params.index.str.contains('Period')] * 100\n",
    "    p.name = \"P value\"\n",
    "    index = pd.concat([linked, growth, p], axis=1)\n",
    "    \n",
    "    index.index = pd.to_datetime(index.index.str.split(\"'\").str.get(1)).to_period(z)\n",
    "    \n",
    "    # update the price adjustment index to be used for later regressions\n",
    "    price_adjustment_index[hood] = index['Index']  \n",
    "    \n",
    "    last = index.tail(1)\n",
    "    table.append([hood\n",
    "                  ,last.index[0]\n",
    "                  ,round(last.iloc[0]['Growth Rate'] * 100,2)\n",
    "                  ,round(last.iloc[0]['P value'], 2)\n",
    "                  ,n])\n",
    "\n",
    "    index[['Index','P value']].plot(title=hood)\n",
    "    index[['Growth Rate']].plot()\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def g(listing): \n",
    "#     print 'price: ' + `listing.price`\n",
    "#     print 'year: ' + `listing.year`\n",
    "#     print 'hood: ' + listing[boundary]\n",
    "    try:\n",
    "        index_value = price_adjustment_index[listing[boundary]][listing.year] / price_adjustment_index[listing[boundary]][2016]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            index_value = price_adjustment_index['city'][listing.year] / price_adjustment_index['city'][2016]\n",
    "        except KeyError:\n",
    "            index_value = 1\n",
    "#     print index_value\n",
    "    indexed_price = listing.price * index_value\n",
    "#     print 'adj price: ' + `indexed_price`\n",
    "    \n",
    "    return indexed_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexed_price = data.apply(g, axis=1)\n",
    "indexed_price.name = \"indexed_price\"\n",
    "data = pd.concat([data,indexed_price],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[['indexed_price','price','year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "out_of_sample_data = data[data.date > datetime.strptime('2016-09-15','%Y-%m-%d').date()]\n",
    "# out_of_sample_data.reset_index(inplace=True)\n",
    "print \"number of transactions in data: \" + `len(data)` + \"\\texcluding latest \" + `len(out_of_sample_data)` + \" transactions\"\n",
    "in_sample_data = data[~data.date.isin(out_of_sample_data.date)]\n",
    "print \"number of transactions in data after exclusion: \" + `len(in_sample_data)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for z in set(out_of_sample_data.zipcode.unique()).difference(in_sample_data.zipcode.unique()):\n",
    "    print(\"shoving missing zipcode into in_sample_data : ?\", z)\n",
    "    in_sample_data = in_sample_data.append(out_of_sample_data[out_of_sample_data.zipcode == z].head(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dist_to_park unit is 1 degree of latitude or 69 miles north south or 54 miles east west\n",
    "\n",
    "result = sm.ols(formula=\"indexed_price ~ bedrooms + bathrooms + elevation + level + age + has_pool + garage + home_type:zipcode:sqft\", data=in_sample_data).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(set(in_sample_data.zipcode.unique()).difference(out_of_sample_data.zipcode.unique()))\n",
    "\n",
    "print list(set(out_of_sample_data.zipcode.unique()).difference(in_sample_data.zipcode.unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for z in set(in_sample_data.zipcode.unique()).difference(out_of_sample_data.zipcode.unique()):\n",
    "    print(\"shoving missing zipcode into out_of_sample_data : ?\", z)\n",
    "    out_of_sample_data = out_of_sample_data.append(in_sample_data[in_sample_data.zipcode == z].head(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(out_of_sample_data.zipcode.unique())\n",
    "print len(in_sample_data.zipcode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "x = patsy.dmatrix(\"bedrooms + bathrooms + elevation + level + age + has_pool + garage + home_type:zipcode:sqft\", data=out_of_sample_data) \n",
    "p = result.predict(x, transform=False)\n",
    "print 'length of prediction from .predict ' + `len(p)`\n",
    "pprice_out = pd.Series(p)\n",
    "pprice_out.name = \"prediction\"\n",
    "print pprice_out.head()\n",
    "print 'length of prediction price ' + `len(pprice_out)`\n",
    "print 'length of out of sample ' + `len(out_of_sample_data)`\n",
    "errors_out = out_of_sample_data.price.values - pprice_out\n",
    "errors_out.name = 'error'\n",
    "print errors_out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'length of OOS data: ' + `len(out_of_sample_data)`\n",
    "print 'length of prediction: ' + `len(pprice_out)`\n",
    "out_of_sample_data.reset_index(drop=True, inplace=True)\n",
    "out_of_sample_result = pd.concat([out_of_sample_data,pprice_out,errors_out],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'length of out of sample data ' + `len(out_of_sample_result)`\n",
    "print 'length of predicted price of OOS data ' + `len(pprice_out)`\n",
    "print 'length of error of OOS data ' + `len(errors_out)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_of_sample_result[['address','price','prediction','error','zipcode']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_errors_out = (1.0 * out_of_sample_result.error.abs() / out_of_sample_result.price)\n",
    "percent_errors_out.name = 'error'\n",
    "\n",
    "print percent_errors_out.median()\n",
    "\n",
    "hooderrors_out = out_of_sample_result[['zipcode']]\n",
    "hooderrors_out = pd.concat([hooderrors_out,errors_out.abs()],axis=1)\n",
    "hood_group_out = hooderrors_out.groupby('zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_avg_out = hood_group_out.agg([np.median,len])\n",
    "print error_avg_out.head()\n",
    "error_avg_out.sort_values(by=('error','len'),ascending=False,inplace=True)\n",
    "error_avg_out.plot(kind='bar',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = [['zipcode','sfh','multifamily']]\n",
    "table = ListTable()\n",
    "table.append(output[0])\n",
    "\n",
    "for row in data.zipcode.unique():\n",
    "    output_row = [row, '99', '99']\n",
    "    for i in result.params.index:\n",
    "        if 'zipcode' not in i: continue\n",
    "\n",
    "        if 'zipcode[' + row + ']' in i:\n",
    "            if 'home_type[mfh]' in i:\n",
    "                output_row[2] = `result.params[i]`\n",
    "                output.append(output_row)\n",
    "                table.append(output_row)\n",
    "\n",
    "            if 'home_type[sfh]' in i:\n",
    "                output_row[1] = `result.params[i]`\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'rentalnerd_importer/lib/tasks/model_files/'\n",
    "\n",
    "with open(path + 'model_zipcode_' + city + '.csv', 'wb') as csvfile:\n",
    "    hoodwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for i in output:\n",
    "        hoodwriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype = [('Effect', 'S100'), ('Coefficient', float)]\n",
    "\n",
    "with open(path + 'model_features_' + city + '.csv', 'wb') as csvfile:\n",
    "    modelwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    header = ['Effect','Coefficient']\n",
    "    table.append(header)\n",
    "    modelwriter.writerow(header)\n",
    "    modelwriter.writerow(['base_rent', result.params.Intercept])\n",
    "    modelwriter.writerow(['bedrooms', result.params.bedrooms])\n",
    "    modelwriter.writerow(['bathrooms', result.params.bathrooms])\n",
    "    modelwriter.writerow(['elevation', result.params.elevation])\n",
    "    modelwriter.writerow(['level', result.params.level])\n",
    "    modelwriter.writerow(['age', result.params.age])\n",
    "    modelwriter.writerow(['garage', result.params.garage])\n",
    "    modelwriter.writerow(['has_pool', result.params['has_pool[T.True]']])\n",
    "    modelwriter.writerow(['mean square error of residuals', result.mse_resid])\n",
    "\n",
    "result.cov_params().to_csv(path + 'model_covs_' + city + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output = [['district_id','coefficient']]\n",
    "# table = ListTable()\n",
    "# table.append(output[0])\n",
    "\n",
    "# for row in sorted(data.school_district_id.unique()):\n",
    "#     output_row = [row, '99']\n",
    "#     for i in result.params.index:\n",
    "#         if 'school_district_id' not in i: continue\n",
    "\n",
    "#         if 'school_district_id)[T.' + `row` + ']' in i:\n",
    "#             output_row[1] = `result.params[i]`\n",
    "#             output.append(output_row)\n",
    "#             table.append(output_row)\n",
    "\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path + 'model_schools_houston.csv', 'wb') as csvfile:\n",
    "#     schoolswriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "#     for i in output:\n",
    "#         schoolswriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = result.resid\n",
    "errors.name = 'error'\n",
    "pprice = data.indexed_price - errors\n",
    "pprice.name = \"prediction\"\n",
    "data = pd.concat([data, errors], axis=1)\n",
    "data = pd.concat([data, pprice], axis=1)\n",
    "\n",
    "# visualize the relationship between the features and the response using scatterplots\n",
    "errors.sort_values(inplace=True)\n",
    "errors.plot(kind='bar').get_xaxis().set_ticks([])\n",
    "\n",
    "# show errors by neighborhood to see if there are any neighborhoods with funky differences\n",
    "\n",
    "hooderrors = data[['zipcode']]\n",
    "hooderrors = pd.concat([hooderrors,errors.abs()],axis=1)\n",
    "hood_group = hooderrors.groupby('zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_avg = hood_group.agg([np.median,len])\n",
    "error_avg.sort_values(by=('error','len'),ascending=False,inplace=True)\n",
    "error_avg.plot(kind='bar',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show errors by year to see if there are any years with funky differences\n",
    "yearerrors = data[['year']]\n",
    "percent_errors = (1.0 * errors.abs() / data.price)\n",
    "percent_errors.name = 'error'\n",
    "yearerrors = pd.concat([yearerrors,percent_errors],axis=1)\n",
    "\n",
    "year_group = yearerrors.groupby('year')\n",
    "error_avg = year_group.median()\n",
    "error_avg.plot(kind='bar')\n",
    "print error_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
